<!DOCTYPE html>
<html lang="en">
<!-- Template: https://github.com/luost26/academic-homepage -->
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <title>Publications - About Me</title>

    <!-- Stylesheets -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.6.0/css/bootstrap.min.css" integrity="sha512-P5MgMn1jBN01asBgU0z60Qk4QxiXo86+wlFahKrsQf37c9cro517WzVSPPV1tDKzhku2iJ2FVgL67wG03SGnNA==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css" integrity="sha512-DTOQO9RWCH3ppGqcWaEA1BIZOC6xxalwEsw9c2QQeAIftl+Vegovlnee1c9QX4TctnWMn13TZye+giMm8e2LwA==" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.1/css/academicons.min.css" integrity="sha512-b1ASx0WHgVFL5ZQhTgiPWX+68KjS38Jk87jg7pe+qC7q9YkEtFq0z7xCglv7qGIs/68d3mAp+StfC8WKC5SSAg==" crossorigin="anonymous" />
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Lato:ital,wght@0,300;0,400;0,700;0,900;1,300;1,400;1,700;1,900&family=Raleway:ital,wght@0,300;0,400;0,500;1,300;1,400;1,500&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css" integrity="sha384-nB0miv6/jRmo5UMMR1wu3Gz6NLsoTkbqJghGIsx//Rlm+ZU03BU6SQNC66uf4l5+" crossorigin="anonymous">
    <link rel="stylesheet" href="/assets/css/global.css">
</head>
<body class="bg-light" data-spy="scroll" data-target="#navbar-year" data-offset="100">
    <nav class="navbar navbar-expand-sm navbar-light bg-light fixed-top mb-5 shadow-sm">
    <div class="container">
        <a class="navbar-brand"><strong>About Me</strong></a>
        <button class="navbar-toggler" style="font-size: 1em; padding: 0.5em;" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
            <i class="fas fa-map"></i> Menu
        </button>

        <div class="collapse navbar-collapse" id="navbarResponsive">
            <ul class="navbar-nav ml-auto">
                
                <li class="nav-item ">
                    <a class="nav-link" href="/">Home</a>
                </li>
                
                <li class="nav-item active">
                    <a class="nav-link" href="/publications">Publications</a>
                </li>
                
            </ul>
        </div>
    </div>
</nav>

    <div class="container">
        

<div class="row">
    <div class="col-12 col-lg-10">
        
        
        <h2 class="pt-4" id="year-2025">2025</h2>
        <div class="my-0 p-0 bg-white shadow-sm rounded-sm">
            
                
<div class="d-none d-md-block">
    <div class="row no-gutters border-bottom border-gray">
        <!-- <div class="col-md-3 col-xl-2 mb-md-0 p-md-3"> -->
            <!--<img data-src="/assets/images/covers/img_magpie.png" alt="Small Models Struggle to Learn from Strong Reasoners" class="lazy w-100 rounded-sm" src="/assets/images/empty_300x200.png">-->
        <!-- </div> -->
        
        <!-- <div class="col-md-12 offset-md-1 col-xl-8 offset-xl-2 p-3 pl-md-0">-->
            <div class="col-12 p-3 pl-md-0 mx-3"> 
            <h5 class="mt-0 mb-1 font-weight-normal">Small Models Struggle to Learn from Strong Reasoners</h5>
            <p class="mt-0 mb-0 small"><span class="text-body">
            Yuetai Li, </span><span class="text-body">
            Xiang Yue, </span><span class="text-body">
            Zhangchen Xu, </span><span class="text-body">
            <strong>Fengqing Jiang</strong>, </span><span class="text-body">
            Luyao Niu, </span><span class="text-body">
            Bill Yuchen Lin, </span><span class="text-body">
            Bhaskar Ramasubramanian, </span><span class="text-body">
            Radha Poovendran</span></p>
            <p class="mt-0 mb-0 small"><i>Preprint</i><strong></strong> </p>
            <!-- <p class="mt-0 mb-0 small text-muted">This paper reveals that small models (≤3B parameters) struggle with long chain-of-thought reasoning and instead perform better with shorter, simpler reasoning chains that align with their learning capacity. To address this, the authors introduce Mix Distillation—a method that blends long and short reasoning examples—which significantly boosts small model performance compared to using either type alone.</p> -->
            <details class="mt-0 mb-0 small text-muted">
                <summary>TL;DR</summary>
                <p>This paper reveals that small models (≤3B parameters) struggle with long chain-of-thought reasoning and instead perform better with shorter, simpler reasoning chains that align with their learning capacity. To address this, the authors introduce Mix Distillation—a method that blends long and short reasoning examples—which significantly boosts small model performance compared to using either type alone.</p>
              </details>
            
            <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                <!-- 
                
                <a target="_blank" href="https://arxiv.org/abs/2502.12143">[Preprint]</a>
                
                 -->
                
                
                    <a target="_blank" href="https://arxiv.org/abs/2502.12143"> Preprint </a>
                
                
                
            </p>
            

            

        </div>
    </div>
</div>

<!-- <div class="row no-gutters d-md-none border-bottom border-gray rounded-top  lazy" data-src="/assets/images/covers/img_magpie.png">
    <div class="w-100" style="background-color: rgba(255,255,255,0.9);">
        <div class="d-flex align-items-start flex-column py-3 px-4">
            <div class="mb-auto"></div>
            <div>
                <h5 class="mt-0 mb-1 font-weight-normal">Small Models Struggle to Learn from Strong Reasoners</h5>
                <p class="mt-0 mb-0 small"><span class="text-body">
            Yuetai Li, </span><span class="text-body">
            Xiang Yue, </span><span class="text-body">
            Zhangchen Xu, </span><span class="text-body">
            <strong>Fengqing Jiang</strong>, </span><span class="text-body">
            Luyao Niu, </span><span class="text-body">
            Bill Yuchen Lin, </span><span class="text-body">
            Bhaskar Ramasubramanian, </span><span class="text-body">
            Radha Poovendran</span></p>
                <p class="mt-0 mb-0 small"><i>Preprint</i> 2025 </p>
                <p class="mt-0 mb-0 small text-muted">This paper reveals that small models (≤3B parameters) struggle with long chain-of-thought reasoning and instead perform better with shorter, simpler reasoning chains that align with their learning capacity. To address this, the authors introduce Mix Distillation—a method that blends long and short reasoning examples—which significantly boosts small model performance compared to using either type alone.</p>
                
                <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                    
                    
                    <a target="_blank" href="https://arxiv.org/abs/2502.12143">[Preprint]</a>
                    
                    
                </p>
            </div>
        </div>
    </div>

</div> -->

<div class="row no-gutters d-md-none 
     border-bottom 
     border-gray 
     rounded-top 
     ">

  <!-- Full-width column with side margins -->
  <div class="col-12 p-3 pl-md-0 mx-3">

    <h5 class="mt-0 mb-1 font-weight-normal">Small Models Struggle to Learn from Strong Reasoners</h5>
    <p class="mt-0 mb-0 small">
      <span class="text-body">
            Yuetai Li, </span><span class="text-body">
            Xiang Yue, </span><span class="text-body">
            Zhangchen Xu, </span><span class="text-body">
            <strong>Fengqing Jiang</strong>, </span><span class="text-body">
            Luyao Niu, </span><span class="text-body">
            Bill Yuchen Lin, </span><span class="text-body">
            Bhaskar Ramasubramanian, </span><span class="text-body">
            Radha Poovendran</span>
    </p>
    
    <p class="mt-0 mb-0 small">
      <i>Preprint</i><strong></strong>
      
    </p>

    <!-- Foldable abstract -->
    <details class="mt-0 mb-0 small text-muted">
      <summary>TL;DR</summary>
      <p>This paper reveals that small models (≤3B parameters) struggle with long chain-of-thought reasoning and instead perform better with shorter, simpler reasoning chains that align with their learning capacity. To address this, the authors introduce Mix Distillation—a method that blends long and short reasoning examples—which significantly boosts small model performance compared to using either type alone.</p>
    </details>

    <!-- Links with separator -->
    <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
      
        
          <a target="_blank" href="https://arxiv.org/abs/2502.12143">Preprint</a>
        
        
      
    </p>

    <!-- Optional media coverage -->
    

  </div><!-- /.col-12 -->
</div><!-- /.row -->
            
                
<div class="d-none d-md-block">
    <div class="row no-gutters border-bottom border-gray">
        <!-- <div class="col-md-3 col-xl-2 mb-md-0 p-md-3"> -->
            <!--<img data-src="/assets/images/covers/img_magpie.png" alt="SafeChain: Safety of Language Models with Long Chain-of-Thought Reasoning Capabilities" class="lazy w-100 rounded-sm" src="/assets/images/empty_300x200.png">-->
        <!-- </div> -->
        
        <!-- <div class="col-md-12 offset-md-1 col-xl-8 offset-xl-2 p-3 pl-md-0">-->
            <div class="col-12 p-3 pl-md-0 mx-3"> 
            <h5 class="mt-0 mb-1 font-weight-normal">SafeChain: Safety of Language Models with Long Chain-of-Thought Reasoning Capabilities</h5>
            <p class="mt-0 mb-0 small"><span class="text-body">
            <strong>Fengqing Jiang</strong>, </span><span class="text-body">
            Zhangchen Xu, </span><span class="text-body">
            Yuetai Li, </span><span class="text-body">
            Luyao Niu, </span><span class="text-body">
            Zhen Xiang, </span><span class="text-body">
            Bo Li, </span><span class="text-body">
            Bill Yuchen Lin, </span><span class="text-body">
            Radha Poovendran</span></p>
            <p class="mt-0 mb-0 small"><i>Preprint</i><strong></strong> </p>
            <!-- <p class="mt-0 mb-0 small text-muted">Emerging large reasoning models (LRMs), such as DeepSeek-R1 models, leverage long chain-of-thought (CoT) reasoning to generate structured intermediate steps, enhancing their reasoning capabilities. However, long CoT does not inherently guarantee safe outputs, potentially leading to harmful consequences such as the introduction of security vulnerabilities in code or the spread of misinformation. Current research on large language model (LLM) safety usually focuses on short-answer responses, overlooking the long CoT style outputs of LRMs. To bridge this gap, we conduct a systematic study of LRM safety. First, we investigate safety evaluators calibrated against human annotations. Using our newly developed metrics, we thoroughly assess the safety of 12 state-of-the-art LRMs on StrongReject and WildJailbreak datasets. Our results show that LRMs are not safe compared to their reasoning advance. Further, we perform a fine-grained analysis of the reasoning trace and final answer. We find that three decoding strategies-ZeroThink, LessThink, and MoreThink-can improve model safety without additional training. However, these strategies either use constrained reasoning traces or incur high inference costs. To better strengthen LRM safety, we introduce SafeChain, the first-of-its-kind safety training dataset in CoT style. We fine-tune two LRMs with SafeChain, showing that it not only enhances model safety but also preserves performance across 6 reasoning benchmarks.</p> -->
            <details class="mt-0 mb-0 small text-muted">
                <summary>TL;DR</summary>
                <p>Emerging large reasoning models (LRMs), such as DeepSeek-R1 models, leverage long chain-of-thought (CoT) reasoning to generate structured intermediate steps, enhancing their reasoning capabilities. However, long CoT does not inherently guarantee safe outputs, potentially leading to harmful consequences such as the introduction of security vulnerabilities in code or the spread of misinformation. Current research on large language model (LLM) safety usually focuses on short-answer responses, overlooking the long CoT style outputs of LRMs. To bridge this gap, we conduct a systematic study of LRM safety. First, we investigate safety evaluators calibrated against human annotations. Using our newly developed metrics, we thoroughly assess the safety of 12 state-of-the-art LRMs on StrongReject and WildJailbreak datasets. Our results show that LRMs are not safe compared to their reasoning advance. Further, we perform a fine-grained analysis of the reasoning trace and final answer. We find that three decoding strategies-ZeroThink, LessThink, and MoreThink-can improve model safety without additional training. However, these strategies either use constrained reasoning traces or incur high inference costs. To better strengthen LRM safety, we introduce SafeChain, the first-of-its-kind safety training dataset in CoT style. We fine-tune two LRMs with SafeChain, showing that it not only enhances model safety but also preserves performance across 6 reasoning benchmarks.</p>
              </details>
            
            <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                <!-- 
                
                <a target="_blank" href="https://arxiv.org/abs/2502.12025">[Preprint]</a>
                
                
                
                <a target="_blank" href="https://safe-chain.github.io/">[Project Website]</a>
                
                
                
                <a target="_blank" href="https://huggingface.co/datasets/UWNSL/SafeChain">[HuggingFace Dataset]</a>
                
                 -->
                
                
                    <a target="_blank" href="https://arxiv.org/abs/2502.12025"> Preprint </a>
                
                 | 
                
                
                    <a target="_blank" href="https://safe-chain.github.io/"> Project Website </a>
                
                 | 
                
                
                    <a target="_blank" href="https://huggingface.co/datasets/UWNSL/SafeChain"> HuggingFace Dataset </a>
                
                
                
            </p>
            

            

        </div>
    </div>
</div>

<!-- <div class="row no-gutters d-md-none border-bottom border-gray   lazy" data-src="/assets/images/covers/img_magpie.png">
    <div class="w-100" style="background-color: rgba(255,255,255,0.9);">
        <div class="d-flex align-items-start flex-column py-3 px-4">
            <div class="mb-auto"></div>
            <div>
                <h5 class="mt-0 mb-1 font-weight-normal">SafeChain: Safety of Language Models with Long Chain-of-Thought Reasoning Capabilities</h5>
                <p class="mt-0 mb-0 small"><span class="text-body">
            <strong>Fengqing Jiang</strong>, </span><span class="text-body">
            Zhangchen Xu, </span><span class="text-body">
            Yuetai Li, </span><span class="text-body">
            Luyao Niu, </span><span class="text-body">
            Zhen Xiang, </span><span class="text-body">
            Bo Li, </span><span class="text-body">
            Bill Yuchen Lin, </span><span class="text-body">
            Radha Poovendran</span></p>
                <p class="mt-0 mb-0 small"><i>Preprint</i> 2025 </p>
                <p class="mt-0 mb-0 small text-muted">Emerging large reasoning models (LRMs), such as DeepSeek-R1 models, leverage long chain-of-thought (CoT) reasoning to generate structured intermediate steps, enhancing their reasoning capabilities. However, long CoT does not inherently guarantee safe outputs, potentially leading to harmful consequences such as the introduction of security vulnerabilities in code or the spread of misinformation. Current research on large language model (LLM) safety usually focuses on short-answer responses, overlooking the long CoT style outputs of LRMs. To bridge this gap, we conduct a systematic study of LRM safety. First, we investigate safety evaluators calibrated against human annotations. Using our newly developed metrics, we thoroughly assess the safety of 12 state-of-the-art LRMs on StrongReject and WildJailbreak datasets. Our results show that LRMs are not safe compared to their reasoning advance. Further, we perform a fine-grained analysis of the reasoning trace and final answer. We find that three decoding strategies-ZeroThink, LessThink, and MoreThink-can improve model safety without additional training. However, these strategies either use constrained reasoning traces or incur high inference costs. To better strengthen LRM safety, we introduce SafeChain, the first-of-its-kind safety training dataset in CoT style. We fine-tune two LRMs with SafeChain, showing that it not only enhances model safety but also preserves performance across 6 reasoning benchmarks.</p>
                
                <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                    
                    
                    <a target="_blank" href="https://arxiv.org/abs/2502.12025">[Preprint]</a>
                    
                    
                    
                    <a target="_blank" href="https://safe-chain.github.io/">[Project Website]</a>
                    
                    
                    
                    <a target="_blank" href="https://huggingface.co/datasets/UWNSL/SafeChain">[HuggingFace Dataset]</a>
                    
                    
                </p>
            </div>
        </div>
    </div>

</div> -->

<div class="row no-gutters d-md-none 
     border-bottom 
     border-gray 
      
     ">

  <!-- Full-width column with side margins -->
  <div class="col-12 p-3 pl-md-0 mx-3">

    <h5 class="mt-0 mb-1 font-weight-normal">SafeChain: Safety of Language Models with Long Chain-of-Thought Reasoning Capabilities</h5>
    <p class="mt-0 mb-0 small">
      <span class="text-body">
            <strong>Fengqing Jiang</strong>, </span><span class="text-body">
            Zhangchen Xu, </span><span class="text-body">
            Yuetai Li, </span><span class="text-body">
            Luyao Niu, </span><span class="text-body">
            Zhen Xiang, </span><span class="text-body">
            Bo Li, </span><span class="text-body">
            Bill Yuchen Lin, </span><span class="text-body">
            Radha Poovendran</span>
    </p>
    
    <p class="mt-0 mb-0 small">
      <i>Preprint</i><strong></strong>
      
    </p>

    <!-- Foldable abstract -->
    <details class="mt-0 mb-0 small text-muted">
      <summary>TL;DR</summary>
      <p>Emerging large reasoning models (LRMs), such as DeepSeek-R1 models, leverage long chain-of-thought (CoT) reasoning to generate structured intermediate steps, enhancing their reasoning capabilities. However, long CoT does not inherently guarantee safe outputs, potentially leading to harmful consequences such as the introduction of security vulnerabilities in code or the spread of misinformation. Current research on large language model (LLM) safety usually focuses on short-answer responses, overlooking the long CoT style outputs of LRMs. To bridge this gap, we conduct a systematic study of LRM safety. First, we investigate safety evaluators calibrated against human annotations. Using our newly developed metrics, we thoroughly assess the safety of 12 state-of-the-art LRMs on StrongReject and WildJailbreak datasets. Our results show that LRMs are not safe compared to their reasoning advance. Further, we perform a fine-grained analysis of the reasoning trace and final answer. We find that three decoding strategies-ZeroThink, LessThink, and MoreThink-can improve model safety without additional training. However, these strategies either use constrained reasoning traces or incur high inference costs. To better strengthen LRM safety, we introduce SafeChain, the first-of-its-kind safety training dataset in CoT style. We fine-tune two LRMs with SafeChain, showing that it not only enhances model safety but also preserves performance across 6 reasoning benchmarks.</p>
    </details>

    <!-- Links with separator -->
    <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
      
        
          <a target="_blank" href="https://arxiv.org/abs/2502.12025">Preprint</a>
        
         | 
      
        
          <a target="_blank" href="https://safe-chain.github.io/">Project Website</a>
        
         | 
      
        
          <a target="_blank" href="https://huggingface.co/datasets/UWNSL/SafeChain">HuggingFace Dataset</a>
        
        
      
    </p>

    <!-- Optional media coverage -->
    

  </div><!-- /.col-12 -->
</div><!-- /.row -->
            
                
<div class="d-none d-md-block">
    <div class="row no-gutters border-bottom border-gray">
        <!-- <div class="col-md-3 col-xl-2 mb-md-0 p-md-3"> -->
            <!--<img data-src="/assets/images/covers/img_magpie.png" alt="Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing" class="lazy w-100 rounded-sm" src="/assets/images/empty_300x200.png">-->
        <!-- </div> -->
        
        <!-- <div class="col-md-12 offset-md-1 col-xl-8 offset-xl-2 p-3 pl-md-0">-->
            <div class="col-12 p-3 pl-md-0 mx-3"> 
            <h5 class="mt-0 mb-1 font-weight-normal">Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing</h5>
            <p class="mt-0 mb-0 small"><span class="text-body">
            Zhangchen Xu, </span><span class="text-body">
            <strong>Fengqing Jiang</strong>, </span><span class="text-body">
            Luyao Niu, </span><span class="text-body">
            Yuntian Deng, </span><span class="text-body">
            Radha Poovendran, </span><span class="text-body">
            Yejin Choi, </span><span class="text-body">
            Bill Yuchen Lin</span></p>
            <p class="mt-0 mb-0 small"><i>ICLR 2025</i><strong></strong> </p>
            <!-- <p class="mt-0 mb-0 small text-muted">This paper introduces Magpie, a self-synthesis method that extracts large-scale, high-quality instruction data directly from aligned LLMs by prompting them with partial templates. By generating 4 million instruction-response pairs and filtering them down to 300K high-quality instances, the approach enables fine-tuned models to perform comparably to or even outperform those trained on much larger proprietary datasets, advancing the democratization of AI alignment.</p> -->
            <details class="mt-0 mb-0 small text-muted">
                <summary>TL;DR</summary>
                <p>This paper introduces Magpie, a self-synthesis method that extracts large-scale, high-quality instruction data directly from aligned LLMs by prompting them with partial templates. By generating 4 million instruction-response pairs and filtering them down to 300K high-quality instances, the approach enables fine-tuned models to perform comparably to or even outperform those trained on much larger proprietary datasets, advancing the democratization of AI alignment.</p>
              </details>
            
            <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                <!-- 
                
                <a target="_blank" href="https://arxiv.org/abs/2406.08464">[Preprint]</a>
                
                
                
                <a target="_blank" href="https://magpie-align.github.io/">[Project Website]</a>
                
                
                
                <a target="_blank" href="https://github.com/magpie-align/magpie">[Code]</a>
                
                
                
                <a target="_blank" href="https://huggingface.co/Magpie-Align">[HuggingFace Dataset]</a>
                
                
                
                <a target="_blank" href="https://huggingface.co/spaces/davanstrien/magpie">[DEMO]</a>
                
                 -->
                
                
                    <a target="_blank" href="https://arxiv.org/abs/2406.08464"> Preprint </a>
                
                 | 
                
                
                    <a target="_blank" href="https://magpie-align.github.io/"> Project Website </a>
                
                 | 
                
                
                    <a target="_blank" href="https://github.com/magpie-align/magpie"> Code </a>
                
                 | 
                
                
                    <a target="_blank" href="https://huggingface.co/Magpie-Align"> HuggingFace Dataset </a>
                
                 | 
                
                
                    <a target="_blank" href="https://huggingface.co/spaces/davanstrien/magpie"> DEMO </a>
                
                
                
            </p>
            

            
            <p class="small pb-0 mb-0 lh-125 text-muted">
            <strong>Media Coverage:</strong>
            <!-- 
                <a target="_blank" href="https://mp.weixin.qq.com/s/VLquLR3sFB4k8kaWYH2bGg"><u>新智元</u></a>
             -->

            
            
                <a target="_blank" href="https://mp.weixin.qq.com/s/VLquLR3sFB4k8kaWYH2bGg"> 新智元 </a>
            
            
            
            </p>
            

        </div>
    </div>
</div>

<!-- <div class="row no-gutters d-md-none border-bottom border-gray   lazy" data-src="/assets/images/covers/img_magpie.png">
    <div class="w-100" style="background-color: rgba(255,255,255,0.9);">
        <div class="d-flex align-items-start flex-column py-3 px-4">
            <div class="mb-auto"></div>
            <div>
                <h5 class="mt-0 mb-1 font-weight-normal">Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing</h5>
                <p class="mt-0 mb-0 small"><span class="text-body">
            Zhangchen Xu, </span><span class="text-body">
            <strong>Fengqing Jiang</strong>, </span><span class="text-body">
            Luyao Niu, </span><span class="text-body">
            Yuntian Deng, </span><span class="text-body">
            Radha Poovendran, </span><span class="text-body">
            Yejin Choi, </span><span class="text-body">
            Bill Yuchen Lin</span></p>
                <p class="mt-0 mb-0 small"><i>ICLR 2025</i> 2025 </p>
                <p class="mt-0 mb-0 small text-muted">This paper introduces Magpie, a self-synthesis method that extracts large-scale, high-quality instruction data directly from aligned LLMs by prompting them with partial templates. By generating 4 million instruction-response pairs and filtering them down to 300K high-quality instances, the approach enables fine-tuned models to perform comparably to or even outperform those trained on much larger proprietary datasets, advancing the democratization of AI alignment.</p>
                
                <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                    
                    
                    <a target="_blank" href="https://arxiv.org/abs/2406.08464">[Preprint]</a>
                    
                    
                    
                    <a target="_blank" href="https://magpie-align.github.io/">[Project Website]</a>
                    
                    
                    
                    <a target="_blank" href="https://github.com/magpie-align/magpie">[Code]</a>
                    
                    
                    
                    <a target="_blank" href="https://huggingface.co/Magpie-Align">[HuggingFace Dataset]</a>
                    
                    
                    
                    <a target="_blank" href="https://huggingface.co/spaces/davanstrien/magpie">[DEMO]</a>
                    
                    
                </p>
            </div>
        </div>
    </div>

</div> -->

<div class="row no-gutters d-md-none 
     border-bottom 
     border-gray 
      
     ">

  <!-- Full-width column with side margins -->
  <div class="col-12 p-3 pl-md-0 mx-3">

    <h5 class="mt-0 mb-1 font-weight-normal">Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing</h5>
    <p class="mt-0 mb-0 small">
      <span class="text-body">
            Zhangchen Xu, </span><span class="text-body">
            <strong>Fengqing Jiang</strong>, </span><span class="text-body">
            Luyao Niu, </span><span class="text-body">
            Yuntian Deng, </span><span class="text-body">
            Radha Poovendran, </span><span class="text-body">
            Yejin Choi, </span><span class="text-body">
            Bill Yuchen Lin</span>
    </p>
    
    <p class="mt-0 mb-0 small">
      <i>ICLR 2025</i><strong></strong>
      
    </p>

    <!-- Foldable abstract -->
    <details class="mt-0 mb-0 small text-muted">
      <summary>TL;DR</summary>
      <p>This paper introduces Magpie, a self-synthesis method that extracts large-scale, high-quality instruction data directly from aligned LLMs by prompting them with partial templates. By generating 4 million instruction-response pairs and filtering them down to 300K high-quality instances, the approach enables fine-tuned models to perform comparably to or even outperform those trained on much larger proprietary datasets, advancing the democratization of AI alignment.</p>
    </details>

    <!-- Links with separator -->
    <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
      
        
          <a target="_blank" href="https://arxiv.org/abs/2406.08464">Preprint</a>
        
         | 
      
        
          <a target="_blank" href="https://magpie-align.github.io/">Project Website</a>
        
         | 
      
        
          <a target="_blank" href="https://github.com/magpie-align/magpie">Code</a>
        
         | 
      
        
          <a target="_blank" href="https://huggingface.co/Magpie-Align">HuggingFace Dataset</a>
        
         | 
      
        
          <a target="_blank" href="https://huggingface.co/spaces/davanstrien/magpie">DEMO</a>
        
        
      
    </p>

    <!-- Optional media coverage -->
    
    <p class="small pb-0 mb-0 lh-125 text-muted">
      <strong>Media Coverage:</strong>
      
        
          <a target="_blank" href="https://mp.weixin.qq.com/s/VLquLR3sFB4k8kaWYH2bGg">新智元</a>
        
        
      
    </p>
    

  </div><!-- /.col-12 -->
</div><!-- /.row -->
            
                
<div class="d-none d-md-block">
    <div class="row no-gutters border-bottom border-gray">
        <!-- <div class="col-md-3 col-xl-2 mb-md-0 p-md-3"> -->
            <!--<img data-src="/assets/images/covers/img_lmp.png" alt="Stronger Models are NOT Stronger Teachers for Instruction Tuning" class="lazy w-100 rounded-sm" src="/assets/images/empty_300x200.png">-->
        <!-- </div> -->
        
        <!-- <div class="col-md-12 offset-md-1 col-xl-8 offset-xl-2 p-3 pl-md-0">-->
            <div class="col-12 p-3 pl-md-0 mx-3"> 
            <h5 class="mt-0 mb-1 font-weight-normal">Stronger Models are NOT Stronger Teachers for Instruction Tuning</h5>
            <p class="mt-0 mb-0 small"><span class="text-body">
            Zhangchen Xu, </span><span class="text-body">
            <strong>Fengqing Jiang</strong>, </span><span class="text-body">
            Luyao Niu, </span><span class="text-body">
            Bill Yuchen Lin, </span><span class="text-body">
            Radha Poovendran</span></p>
            <p class="mt-0 mb-0 small"><i>NAACL 2025</i><strong></strong> </p>
            <!-- <p class="mt-0 mb-0 small text-muted">This paper challenges the common assumption that larger models are inherently better teachers for instruction tuning, revealing a "Larger Models' Paradox" where stronger models don't necessarily yield better results when fine-tuning smaller models. To address this, the authors propose a novel metric—Compatibility-Adjusted Reward (CAR)—which more accurately predicts the effectiveness of response generators by considering compatibility between teacher and base models, outperforming existing metrics across various experiments.  </p> -->
            <details class="mt-0 mb-0 small text-muted">
                <summary>TL;DR</summary>
                <p>This paper challenges the common assumption that larger models are inherently better teachers for instruction tuning, revealing a "Larger Models' Paradox" where stronger models don't necessarily yield better results when fine-tuning smaller models. To address this, the authors propose a novel metric—Compatibility-Adjusted Reward (CAR)—which more accurately predicts the effectiveness of response generators by considering compatibility between teacher and base models, outperforming existing metrics across various experiments.  </p>
              </details>
            
            <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                <!-- 
                
                <a target="_blank" href="https://arxiv.org/abs/2411.07133">[Preprint]</a>
                
                 -->
                
                
                    <a target="_blank" href="https://arxiv.org/abs/2411.07133"> Preprint </a>
                
                
                
            </p>
            

            

        </div>
    </div>
</div>

<!-- <div class="row no-gutters d-md-none border-bottom border-gray   lazy" data-src="/assets/images/covers/img_lmp.png">
    <div class="w-100" style="background-color: rgba(255,255,255,0.9);">
        <div class="d-flex align-items-start flex-column py-3 px-4">
            <div class="mb-auto"></div>
            <div>
                <h5 class="mt-0 mb-1 font-weight-normal">Stronger Models are NOT Stronger Teachers for Instruction Tuning</h5>
                <p class="mt-0 mb-0 small"><span class="text-body">
            Zhangchen Xu, </span><span class="text-body">
            <strong>Fengqing Jiang</strong>, </span><span class="text-body">
            Luyao Niu, </span><span class="text-body">
            Bill Yuchen Lin, </span><span class="text-body">
            Radha Poovendran</span></p>
                <p class="mt-0 mb-0 small"><i>NAACL 2025</i> 2025 </p>
                <p class="mt-0 mb-0 small text-muted">This paper challenges the common assumption that larger models are inherently better teachers for instruction tuning, revealing a "Larger Models' Paradox" where stronger models don't necessarily yield better results when fine-tuning smaller models. To address this, the authors propose a novel metric—Compatibility-Adjusted Reward (CAR)—which more accurately predicts the effectiveness of response generators by considering compatibility between teacher and base models, outperforming existing metrics across various experiments.  </p>
                
                <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                    
                    
                    <a target="_blank" href="https://arxiv.org/abs/2411.07133">[Preprint]</a>
                    
                    
                </p>
            </div>
        </div>
    </div>

</div> -->

<div class="row no-gutters d-md-none 
     border-bottom 
     border-gray 
      
     ">

  <!-- Full-width column with side margins -->
  <div class="col-12 p-3 pl-md-0 mx-3">

    <h5 class="mt-0 mb-1 font-weight-normal">Stronger Models are NOT Stronger Teachers for Instruction Tuning</h5>
    <p class="mt-0 mb-0 small">
      <span class="text-body">
            Zhangchen Xu, </span><span class="text-body">
            <strong>Fengqing Jiang</strong>, </span><span class="text-body">
            Luyao Niu, </span><span class="text-body">
            Bill Yuchen Lin, </span><span class="text-body">
            Radha Poovendran</span>
    </p>
    
    <p class="mt-0 mb-0 small">
      <i>NAACL 2025</i><strong></strong>
      
    </p>

    <!-- Foldable abstract -->
    <details class="mt-0 mb-0 small text-muted">
      <summary>TL;DR</summary>
      <p>This paper challenges the common assumption that larger models are inherently better teachers for instruction tuning, revealing a "Larger Models' Paradox" where stronger models don't necessarily yield better results when fine-tuning smaller models. To address this, the authors propose a novel metric—Compatibility-Adjusted Reward (CAR)—which more accurately predicts the effectiveness of response generators by considering compatibility between teacher and base models, outperforming existing metrics across various experiments.  </p>
    </details>

    <!-- Links with separator -->
    <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
      
        
          <a target="_blank" href="https://arxiv.org/abs/2411.07133">Preprint</a>
        
        
      
    </p>

    <!-- Optional media coverage -->
    

  </div><!-- /.col-12 -->
</div><!-- /.row -->
            
                
<div class="d-none d-md-block">
    <div class="row no-gutters  border-gray">
        <!-- <div class="col-md-3 col-xl-2 mb-md-0 p-md-3"> -->
            <!--<img data-src="/assets/images/covers/img_chatbug.png" alt="ChatBug: A Common Vulnerability of Aligned LLMs Induced by Chat Templates" class="lazy w-100 rounded-sm" src="/assets/images/empty_300x200.png">-->
        <!-- </div> -->
        
        <!-- <div class="col-md-12 offset-md-1 col-xl-8 offset-xl-2 p-3 pl-md-0">-->
            <div class="col-12 p-3 pl-md-0 mx-3"> 
            <h5 class="mt-0 mb-1 font-weight-normal">ChatBug: A Common Vulnerability of Aligned LLMs Induced by Chat Templates</h5>
            <p class="mt-0 mb-0 small"><span class="text-body">
            <strong>Fengqing Jiang</strong>, </span><span class="text-body">
            Zhangchen Xu, </span><span class="text-body">
            Luyao Niu, </span><span class="text-body">
            Bill Yuchen Lin, </span><span class="text-body">
            Radha Poovendran</span></p>
            <p class="mt-0 mb-0 small"><i>AAAI 2025 (AIA)</i><strong></strong> </p>
            <!-- <p class="mt-0 mb-0 small text-muted">This paper reveals that using rigid chat templates for instruction tuning can inadvertently introduce a vulnerability, termed ChatBug, which attackers can exploit by deviating from the expected format to bypass safety alignments. The authors demonstrate that ChatBug can trigger unintended responses in multiple state-of-the-art LLMs and, although adversarial training can mitigate this risk, it significantly degrades performance, highlighting a trade-off between safety and helpfulness.</p> -->
            <details class="mt-0 mb-0 small text-muted">
                <summary>TL;DR</summary>
                <p>This paper reveals that using rigid chat templates for instruction tuning can inadvertently introduce a vulnerability, termed ChatBug, which attackers can exploit by deviating from the expected format to bypass safety alignments. The authors demonstrate that ChatBug can trigger unintended responses in multiple state-of-the-art LLMs and, although adversarial training can mitigate this risk, it significantly degrades performance, highlighting a trade-off between safety and helpfulness.</p>
              </details>
            
            <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                <!-- 
                
                <a target="_blank" href="https://arxiv.org/abs/2406.12935">[Full Version]</a>
                
                
                
                <a target="_blank" href="https://github.com/uw-nsl/ChatBug">[Code]</a>
                
                 -->
                
                
                    <a target="_blank" href="https://arxiv.org/abs/2406.12935"> Full Version </a>
                
                 | 
                
                
                    <a target="_blank" href="https://github.com/uw-nsl/ChatBug"> Code </a>
                
                
                
            </p>
            

            

        </div>
    </div>
</div>

<!-- <div class="row no-gutters d-md-none  border-gray  rounded-bottom lazy" data-src="/assets/images/covers/img_chatbug.png">
    <div class="w-100" style="background-color: rgba(255,255,255,0.9);">
        <div class="d-flex align-items-start flex-column py-3 px-4">
            <div class="mb-auto"></div>
            <div>
                <h5 class="mt-0 mb-1 font-weight-normal">ChatBug: A Common Vulnerability of Aligned LLMs Induced by Chat Templates</h5>
                <p class="mt-0 mb-0 small"><span class="text-body">
            <strong>Fengqing Jiang</strong>, </span><span class="text-body">
            Zhangchen Xu, </span><span class="text-body">
            Luyao Niu, </span><span class="text-body">
            Bill Yuchen Lin, </span><span class="text-body">
            Radha Poovendran</span></p>
                <p class="mt-0 mb-0 small"><i>AAAI 2025 (AIA)</i> 2025 </p>
                <p class="mt-0 mb-0 small text-muted">This paper reveals that using rigid chat templates for instruction tuning can inadvertently introduce a vulnerability, termed ChatBug, which attackers can exploit by deviating from the expected format to bypass safety alignments. The authors demonstrate that ChatBug can trigger unintended responses in multiple state-of-the-art LLMs and, although adversarial training can mitigate this risk, it significantly degrades performance, highlighting a trade-off between safety and helpfulness.</p>
                
                <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                    
                    
                    <a target="_blank" href="https://arxiv.org/abs/2406.12935">[Full Version]</a>
                    
                    
                    
                    <a target="_blank" href="https://github.com/uw-nsl/ChatBug">[Code]</a>
                    
                    
                </p>
            </div>
        </div>
    </div>

</div> -->

<div class="row no-gutters d-md-none 
      
     border-gray 
      
     rounded-bottom">

  <!-- Full-width column with side margins -->
  <div class="col-12 p-3 pl-md-0 mx-3">

    <h5 class="mt-0 mb-1 font-weight-normal">ChatBug: A Common Vulnerability of Aligned LLMs Induced by Chat Templates</h5>
    <p class="mt-0 mb-0 small">
      <span class="text-body">
            <strong>Fengqing Jiang</strong>, </span><span class="text-body">
            Zhangchen Xu, </span><span class="text-body">
            Luyao Niu, </span><span class="text-body">
            Bill Yuchen Lin, </span><span class="text-body">
            Radha Poovendran</span>
    </p>
    
    <p class="mt-0 mb-0 small">
      <i>AAAI 2025 (AIA)</i><strong></strong>
      
    </p>

    <!-- Foldable abstract -->
    <details class="mt-0 mb-0 small text-muted">
      <summary>TL;DR</summary>
      <p>This paper reveals that using rigid chat templates for instruction tuning can inadvertently introduce a vulnerability, termed ChatBug, which attackers can exploit by deviating from the expected format to bypass safety alignments. The authors demonstrate that ChatBug can trigger unintended responses in multiple state-of-the-art LLMs and, although adversarial training can mitigate this risk, it significantly degrades performance, highlighting a trade-off between safety and helpfulness.</p>
    </details>

    <!-- Links with separator -->
    <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
      
        
          <a target="_blank" href="https://arxiv.org/abs/2406.12935">Full Version</a>
        
         | 
      
        
          <a target="_blank" href="https://github.com/uw-nsl/ChatBug">Code</a>
        
        
      
    </p>

    <!-- Optional media coverage -->
    

  </div><!-- /.col-12 -->
</div><!-- /.row -->
            
        </div>
        
        
        <h2 class="pt-4" id="year-2024">2024</h2>
        <div class="my-0 p-0 bg-white shadow-sm rounded-sm">
            
                
<div class="d-none d-md-block">
    <div class="row no-gutters border-bottom border-gray">
        <!-- <div class="col-md-3 col-xl-2 mb-md-0 p-md-3"> -->
            <!--<img data-src="/assets/images/covers/img_cleangen.png" alt="CleanGen: Mitigating Backdoor Attacks for Generation Tasks in Large Language Models" class="lazy w-100 rounded-sm" src="/assets/images/empty_300x200.png">-->
        <!-- </div> -->
        
        <!-- <div class="col-md-12 offset-md-1 col-xl-8 offset-xl-2 p-3 pl-md-0">-->
            <div class="col-12 p-3 pl-md-0 mx-3"> 
            <h5 class="mt-0 mb-1 font-weight-normal">CleanGen: Mitigating Backdoor Attacks for Generation Tasks in Large Language Models</h5>
            <p class="mt-0 mb-0 small"><span class="text-body">
            Yuetai Li, </span><span class="text-body">
            Zhangchen Xu, </span><span class="text-body">
            <strong>Fengqing Jiang</strong>, </span><span class="text-body">
            Luyao Niu, </span><span class="text-body">
            Dinuka Sahabandu, </span><span class="text-body">
            Bhaskar Ramasubramanian, </span><span class="text-body">
            Radha Poovendran</span></p>
            <p class="mt-0 mb-0 small"><i>EMNLP 2024</i><strong></strong> </p>
            <!-- <p class="mt-0 mb-0 small text-muted">This paper introduces CleanGen, an inference-time defense for LLMs that mitigates backdoor attacks by identifying and replacing suspicious tokens—those with unusually high probabilities in compromised models—with tokens from a trusted, uncompromised LLM. Empirical evaluations demonstrate that CleanGen significantly reduces attack success rates across multiple backdoor attacks while preserving the quality and helpfulness of responses with minimal computational overhead.</p> -->
            <details class="mt-0 mb-0 small text-muted">
                <summary>TL;DR</summary>
                <p>This paper introduces CleanGen, an inference-time defense for LLMs that mitigates backdoor attacks by identifying and replacing suspicious tokens—those with unusually high probabilities in compromised models—with tokens from a trusted, uncompromised LLM. Empirical evaluations demonstrate that CleanGen significantly reduces attack success rates across multiple backdoor attacks while preserving the quality and helpfulness of responses with minimal computational overhead.</p>
              </details>
            
            <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                <!-- 
                
                <a target="_blank" href="https://aclanthology.org/2024.emnlp-main.514/">[Paper]</a>
                
                
                
                <a target="_blank" href="https://github.com/uw-nsl/CleanGen">[Code]</a>
                
                 -->
                
                
                    <a target="_blank" href="https://aclanthology.org/2024.emnlp-main.514/"> Paper </a>
                
                 | 
                
                
                    <a target="_blank" href="https://github.com/uw-nsl/CleanGen"> Code </a>
                
                
                
            </p>
            

            

        </div>
    </div>
</div>

<!-- <div class="row no-gutters d-md-none border-bottom border-gray rounded-top  lazy" data-src="/assets/images/covers/img_cleangen.png">
    <div class="w-100" style="background-color: rgba(255,255,255,0.9);">
        <div class="d-flex align-items-start flex-column py-3 px-4">
            <div class="mb-auto"></div>
            <div>
                <h5 class="mt-0 mb-1 font-weight-normal">CleanGen: Mitigating Backdoor Attacks for Generation Tasks in Large Language Models</h5>
                <p class="mt-0 mb-0 small"><span class="text-body">
            Yuetai Li, </span><span class="text-body">
            Zhangchen Xu, </span><span class="text-body">
            <strong>Fengqing Jiang</strong>, </span><span class="text-body">
            Luyao Niu, </span><span class="text-body">
            Dinuka Sahabandu, </span><span class="text-body">
            Bhaskar Ramasubramanian, </span><span class="text-body">
            Radha Poovendran</span></p>
                <p class="mt-0 mb-0 small"><i>EMNLP 2024</i> 2024 </p>
                <p class="mt-0 mb-0 small text-muted">This paper introduces CleanGen, an inference-time defense for LLMs that mitigates backdoor attacks by identifying and replacing suspicious tokens—those with unusually high probabilities in compromised models—with tokens from a trusted, uncompromised LLM. Empirical evaluations demonstrate that CleanGen significantly reduces attack success rates across multiple backdoor attacks while preserving the quality and helpfulness of responses with minimal computational overhead.</p>
                
                <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                    
                    
                    <a target="_blank" href="https://aclanthology.org/2024.emnlp-main.514/">[Paper]</a>
                    
                    
                    
                    <a target="_blank" href="https://github.com/uw-nsl/CleanGen">[Code]</a>
                    
                    
                </p>
            </div>
        </div>
    </div>

</div> -->

<div class="row no-gutters d-md-none 
     border-bottom 
     border-gray 
     rounded-top 
     ">

  <!-- Full-width column with side margins -->
  <div class="col-12 p-3 pl-md-0 mx-3">

    <h5 class="mt-0 mb-1 font-weight-normal">CleanGen: Mitigating Backdoor Attacks for Generation Tasks in Large Language Models</h5>
    <p class="mt-0 mb-0 small">
      <span class="text-body">
            Yuetai Li, </span><span class="text-body">
            Zhangchen Xu, </span><span class="text-body">
            <strong>Fengqing Jiang</strong>, </span><span class="text-body">
            Luyao Niu, </span><span class="text-body">
            Dinuka Sahabandu, </span><span class="text-body">
            Bhaskar Ramasubramanian, </span><span class="text-body">
            Radha Poovendran</span>
    </p>
    
    <p class="mt-0 mb-0 small">
      <i>EMNLP 2024</i><strong></strong>
      
    </p>

    <!-- Foldable abstract -->
    <details class="mt-0 mb-0 small text-muted">
      <summary>TL;DR</summary>
      <p>This paper introduces CleanGen, an inference-time defense for LLMs that mitigates backdoor attacks by identifying and replacing suspicious tokens—those with unusually high probabilities in compromised models—with tokens from a trusted, uncompromised LLM. Empirical evaluations demonstrate that CleanGen significantly reduces attack success rates across multiple backdoor attacks while preserving the quality and helpfulness of responses with minimal computational overhead.</p>
    </details>

    <!-- Links with separator -->
    <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
      
        
          <a target="_blank" href="https://aclanthology.org/2024.emnlp-main.514/">Paper</a>
        
         | 
      
        
          <a target="_blank" href="https://github.com/uw-nsl/CleanGen">Code</a>
        
        
      
    </p>

    <!-- Optional media coverage -->
    

  </div><!-- /.col-12 -->
</div><!-- /.row -->
            
                
<div class="d-none d-md-block">
    <div class="row no-gutters border-bottom border-gray">
        <!-- <div class="col-md-3 col-xl-2 mb-md-0 p-md-3"> -->
            <!--<img data-src="/assets/images/covers/img_ace.png" alt="ACE: A Model Poisoning Attack on Contribution Evaluation Methods in Federated Learning" class="lazy w-100 rounded-sm" src="/assets/images/empty_300x200.png">-->
        <!-- </div> -->
        
        <!-- <div class="col-md-12 offset-md-1 col-xl-8 offset-xl-2 p-3 pl-md-0">-->
            <div class="col-12 p-3 pl-md-0 mx-3"> 
            <h5 class="mt-0 mb-1 font-weight-normal">ACE: A Model Poisoning Attack on Contribution Evaluation Methods in Federated Learning</h5>
            <p class="mt-0 mb-0 small"><span class="text-body">
            Zhangchen Xu, </span><span class="text-body">
            <strong>Fengqing Jiang</strong>, </span><span class="text-body">
            Luyao Niu, </span><span class="text-body">
            Jinyuan Jia, </span><span class="text-body">
            Bo Li, </span><span class="text-body">
            Radha Poovendran</span></p>
            <p class="mt-0 mb-0 small"><i>USENIX Security 2024</i><strong></strong> </p>
            <!-- <p class="mt-0 mb-0 small text-muted">This paper introduces ACE, the first model poisoning attack targeting contribution evaluation in Federated Learning, allowing malicious clients to falsely boost their perceived contributions despite using low-quality data. Both theoretical and empirical results show that ACE deceives multiple state-of-the-art evaluation methods—while preserving global model accuracy—and that existing countermeasures are insufficient, highlighting the need for more robust defenses.</p> -->
            <details class="mt-0 mb-0 small text-muted">
                <summary>TL;DR</summary>
                <p>This paper introduces ACE, the first model poisoning attack targeting contribution evaluation in Federated Learning, allowing malicious clients to falsely boost their perceived contributions despite using low-quality data. Both theoretical and empirical results show that ACE deceives multiple state-of-the-art evaluation methods—while preserving global model accuracy—and that existing countermeasures are insufficient, highlighting the need for more robust defenses.</p>
              </details>
            
            <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                <!-- 
                
                <a target="_blank" href="https://dl.acm.org/doi/10.5555/3698900.3699134">[Paper]</a>
                
                
                
                <a target="_blank" href="https://arxiv.org/abs/2405.20975">[Full Version]</a>
                
                 -->
                
                
                    <a target="_blank" href="https://dl.acm.org/doi/10.5555/3698900.3699134"> Paper </a>
                
                 | 
                
                
                    <a target="_blank" href="https://arxiv.org/abs/2405.20975"> Full Version </a>
                
                
                
            </p>
            

            

        </div>
    </div>
</div>

<!-- <div class="row no-gutters d-md-none border-bottom border-gray   lazy" data-src="/assets/images/covers/img_ace.png">
    <div class="w-100" style="background-color: rgba(255,255,255,0.9);">
        <div class="d-flex align-items-start flex-column py-3 px-4">
            <div class="mb-auto"></div>
            <div>
                <h5 class="mt-0 mb-1 font-weight-normal">ACE: A Model Poisoning Attack on Contribution Evaluation Methods in Federated Learning</h5>
                <p class="mt-0 mb-0 small"><span class="text-body">
            Zhangchen Xu, </span><span class="text-body">
            <strong>Fengqing Jiang</strong>, </span><span class="text-body">
            Luyao Niu, </span><span class="text-body">
            Jinyuan Jia, </span><span class="text-body">
            Bo Li, </span><span class="text-body">
            Radha Poovendran</span></p>
                <p class="mt-0 mb-0 small"><i>USENIX Security 2024</i> 2024 </p>
                <p class="mt-0 mb-0 small text-muted">This paper introduces ACE, the first model poisoning attack targeting contribution evaluation in Federated Learning, allowing malicious clients to falsely boost their perceived contributions despite using low-quality data. Both theoretical and empirical results show that ACE deceives multiple state-of-the-art evaluation methods—while preserving global model accuracy—and that existing countermeasures are insufficient, highlighting the need for more robust defenses.</p>
                
                <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                    
                    
                    <a target="_blank" href="https://dl.acm.org/doi/10.5555/3698900.3699134">[Paper]</a>
                    
                    
                    
                    <a target="_blank" href="https://arxiv.org/abs/2405.20975">[Full Version]</a>
                    
                    
                </p>
            </div>
        </div>
    </div>

</div> -->

<div class="row no-gutters d-md-none 
     border-bottom 
     border-gray 
      
     ">

  <!-- Full-width column with side margins -->
  <div class="col-12 p-3 pl-md-0 mx-3">

    <h5 class="mt-0 mb-1 font-weight-normal">ACE: A Model Poisoning Attack on Contribution Evaluation Methods in Federated Learning</h5>
    <p class="mt-0 mb-0 small">
      <span class="text-body">
            Zhangchen Xu, </span><span class="text-body">
            <strong>Fengqing Jiang</strong>, </span><span class="text-body">
            Luyao Niu, </span><span class="text-body">
            Jinyuan Jia, </span><span class="text-body">
            Bo Li, </span><span class="text-body">
            Radha Poovendran</span>
    </p>
    
    <p class="mt-0 mb-0 small">
      <i>USENIX Security 2024</i><strong></strong>
      
    </p>

    <!-- Foldable abstract -->
    <details class="mt-0 mb-0 small text-muted">
      <summary>TL;DR</summary>
      <p>This paper introduces ACE, the first model poisoning attack targeting contribution evaluation in Federated Learning, allowing malicious clients to falsely boost their perceived contributions despite using low-quality data. Both theoretical and empirical results show that ACE deceives multiple state-of-the-art evaluation methods—while preserving global model accuracy—and that existing countermeasures are insufficient, highlighting the need for more robust defenses.</p>
    </details>

    <!-- Links with separator -->
    <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
      
        
          <a target="_blank" href="https://dl.acm.org/doi/10.5555/3698900.3699134">Paper</a>
        
         | 
      
        
          <a target="_blank" href="https://arxiv.org/abs/2405.20975">Full Version</a>
        
        
      
    </p>

    <!-- Optional media coverage -->
    

  </div><!-- /.col-12 -->
</div><!-- /.row -->
            
                
<div class="d-none d-md-block">
    <div class="row no-gutters border-bottom border-gray">
        <!-- <div class="col-md-3 col-xl-2 mb-md-0 p-md-3"> -->
            <!--<img data-src="/assets/images/covers/img_artprompt.png" alt="ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs" class="lazy w-100 rounded-sm" src="/assets/images/empty_300x200.png">-->
        <!-- </div> -->
        
        <!-- <div class="col-md-12 offset-md-1 col-xl-8 offset-xl-2 p-3 pl-md-0">-->
            <div class="col-12 p-3 pl-md-0 mx-3"> 
            <h5 class="mt-0 mb-1 font-weight-normal">ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs</h5>
            <p class="mt-0 mb-0 small"><span class="text-body">
            <strong>Fengqing Jiang</strong>, </span><span class="text-body">
            Zhangchen Xu, </span><span class="text-body">
            Luyao Niu, </span><span class="text-body">
            Zhen Xiang, </span><span class="text-body">
            Bhaskar Ramasubramanian, </span><span class="text-body">
            Bo Li, </span><span class="text-body">
            Radha Poovendran</span></p>
            <p class="mt-0 mb-0 small"><i>ACL 2024</i><strong></strong> </p>
            <!-- <p class="mt-0 mb-0 small text-muted">This paper reveals that current LLM safety methods, which assume text is interpreted only semantically, can be bypassed using ASCII art. The authors introduce ArtPrompt, an ASCII art-based jailbreak attack, and the Vision-in-Text Challenge (ViTC) benchmark to demonstrate that leading LLMs struggle with non-semantic cues, enabling attackers to trigger undesired behaviors with black-box access.</p> -->
            <details class="mt-0 mb-0 small text-muted">
                <summary>TL;DR</summary>
                <p>This paper reveals that current LLM safety methods, which assume text is interpreted only semantically, can be bypassed using ASCII art. The authors introduce ArtPrompt, an ASCII art-based jailbreak attack, and the Vision-in-Text Challenge (ViTC) benchmark to demonstrate that leading LLMs struggle with non-semantic cues, enabling attackers to trigger undesired behaviors with black-box access.</p>
              </details>
            
            <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                <!-- 
                
                <a target="_blank" href="https://aclanthology.org/2024.acl-long.809/">[Paper]</a>
                
                
                
                <a target="_blank" href="https://github.com/uw-nsl/ArtPrompt">[Code]</a>
                
                 -->
                
                
                    <a target="_blank" href="https://aclanthology.org/2024.acl-long.809/"> Paper </a>
                
                 | 
                
                
                    <a target="_blank" href="https://github.com/uw-nsl/ArtPrompt"> Code </a>
                
                
                
            </p>
            

            
            <p class="small pb-0 mb-0 lh-125 text-muted">
            <strong>Media Coverage:</strong>
            <!-- 
                <a target="_blank" href="https://www.deeplearning.ai/the-batch/artprompt-a-technique-that-exploits-ascii-art-to-bypass-llm-safety-measures/"><u>DeepLearning.AI</u></a>
            
                <a target="_blank" href="https://x.com/emollick/status/1763687813386547594"><u>X</u></a>
            
                <a target="_blank" href="https://www.tomshardware.com/tech-industry/artificial-intelligence/researchers-jailbreak-ai-chatbots-with-ascii-art-artprompt-bypasses-safety-measures-to-unlock-malicious-queries"><u>tom's Hardware</u></a>
             -->

            
            
                <a target="_blank" href="https://www.deeplearning.ai/the-batch/artprompt-a-technique-that-exploits-ascii-art-to-bypass-llm-safety-measures/"> DeepLearning.AI </a>
            
             | 
            
            
                <a target="_blank" href="https://x.com/emollick/status/1763687813386547594"> X </a>
            
             | 
            
            
                <a target="_blank" href="https://www.tomshardware.com/tech-industry/artificial-intelligence/researchers-jailbreak-ai-chatbots-with-ascii-art-artprompt-bypasses-safety-measures-to-unlock-malicious-queries"> tom's Hardware </a>
            
            
            
            </p>
            

        </div>
    </div>
</div>

<!-- <div class="row no-gutters d-md-none border-bottom border-gray   lazy" data-src="/assets/images/covers/img_artprompt.png">
    <div class="w-100" style="background-color: rgba(255,255,255,0.9);">
        <div class="d-flex align-items-start flex-column py-3 px-4">
            <div class="mb-auto"></div>
            <div>
                <h5 class="mt-0 mb-1 font-weight-normal">ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs</h5>
                <p class="mt-0 mb-0 small"><span class="text-body">
            <strong>Fengqing Jiang</strong>, </span><span class="text-body">
            Zhangchen Xu, </span><span class="text-body">
            Luyao Niu, </span><span class="text-body">
            Zhen Xiang, </span><span class="text-body">
            Bhaskar Ramasubramanian, </span><span class="text-body">
            Bo Li, </span><span class="text-body">
            Radha Poovendran</span></p>
                <p class="mt-0 mb-0 small"><i>ACL 2024</i> 2024 </p>
                <p class="mt-0 mb-0 small text-muted">This paper reveals that current LLM safety methods, which assume text is interpreted only semantically, can be bypassed using ASCII art. The authors introduce ArtPrompt, an ASCII art-based jailbreak attack, and the Vision-in-Text Challenge (ViTC) benchmark to demonstrate that leading LLMs struggle with non-semantic cues, enabling attackers to trigger undesired behaviors with black-box access.</p>
                
                <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                    
                    
                    <a target="_blank" href="https://aclanthology.org/2024.acl-long.809/">[Paper]</a>
                    
                    
                    
                    <a target="_blank" href="https://github.com/uw-nsl/ArtPrompt">[Code]</a>
                    
                    
                </p>
            </div>
        </div>
    </div>

</div> -->

<div class="row no-gutters d-md-none 
     border-bottom 
     border-gray 
      
     ">

  <!-- Full-width column with side margins -->
  <div class="col-12 p-3 pl-md-0 mx-3">

    <h5 class="mt-0 mb-1 font-weight-normal">ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs</h5>
    <p class="mt-0 mb-0 small">
      <span class="text-body">
            <strong>Fengqing Jiang</strong>, </span><span class="text-body">
            Zhangchen Xu, </span><span class="text-body">
            Luyao Niu, </span><span class="text-body">
            Zhen Xiang, </span><span class="text-body">
            Bhaskar Ramasubramanian, </span><span class="text-body">
            Bo Li, </span><span class="text-body">
            Radha Poovendran</span>
    </p>
    
    <p class="mt-0 mb-0 small">
      <i>ACL 2024</i><strong></strong>
      
    </p>

    <!-- Foldable abstract -->
    <details class="mt-0 mb-0 small text-muted">
      <summary>TL;DR</summary>
      <p>This paper reveals that current LLM safety methods, which assume text is interpreted only semantically, can be bypassed using ASCII art. The authors introduce ArtPrompt, an ASCII art-based jailbreak attack, and the Vision-in-Text Challenge (ViTC) benchmark to demonstrate that leading LLMs struggle with non-semantic cues, enabling attackers to trigger undesired behaviors with black-box access.</p>
    </details>

    <!-- Links with separator -->
    <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
      
        
          <a target="_blank" href="https://aclanthology.org/2024.acl-long.809/">Paper</a>
        
         | 
      
        
          <a target="_blank" href="https://github.com/uw-nsl/ArtPrompt">Code</a>
        
        
      
    </p>

    <!-- Optional media coverage -->
    
    <p class="small pb-0 mb-0 lh-125 text-muted">
      <strong>Media Coverage:</strong>
      
        
          <a target="_blank" href="https://www.deeplearning.ai/the-batch/artprompt-a-technique-that-exploits-ascii-art-to-bypass-llm-safety-measures/">DeepLearning.AI</a>
        
         | 
      
        
          <a target="_blank" href="https://x.com/emollick/status/1763687813386547594">X</a>
        
         | 
      
        
          <a target="_blank" href="https://www.tomshardware.com/tech-industry/artificial-intelligence/researchers-jailbreak-ai-chatbots-with-ascii-art-artprompt-bypasses-safety-measures-to-unlock-malicious-queries">tom's Hardware</a>
        
        
      
    </p>
    

  </div><!-- /.col-12 -->
</div><!-- /.row -->
            
                
<div class="d-none d-md-block">
    <div class="row no-gutters border-bottom border-gray">
        <!-- <div class="col-md-3 col-xl-2 mb-md-0 p-md-3"> -->
            <!--<img data-src="/assets/images/covers/img_safedecoding.png" alt="SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding" class="lazy w-100 rounded-sm" src="/assets/images/empty_300x200.png">-->
        <!-- </div> -->
        
        <!-- <div class="col-md-12 offset-md-1 col-xl-8 offset-xl-2 p-3 pl-md-0">-->
            <div class="col-12 p-3 pl-md-0 mx-3"> 
            <h5 class="mt-0 mb-1 font-weight-normal">SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding</h5>
            <p class="mt-0 mb-0 small"><span class="text-body">
            Zhangchen Xu, </span><span class="text-body">
            <strong>Fengqing Jiang</strong>, </span><span class="text-body">
            Luyao Niu, </span><span class="text-body">
            Jinyuan Jia, </span><span class="text-body">
            Bill Yuchen Lin, </span><span class="text-body">
            Radha Poovendran</span></p>
            <p class="mt-0 mb-0 small"><i>ACL 2024</i><strong></strong> </p>
            <!-- <p class="mt-0 mb-0 small text-muted">This paper introduces SafeDecoding, a safety-aware decoding strategy for LLMs that leverages token probability insights to reduce the risk of jailbreak attacks. Extensive experiments show that SafeDecoding effectively lowers the success rate and harmfulness of various attacks across multiple LLMs, while maintaining response helpfulness and outperforming existing defense methods.  </p> -->
            <details class="mt-0 mb-0 small text-muted">
                <summary>TL;DR</summary>
                <p>This paper introduces SafeDecoding, a safety-aware decoding strategy for LLMs that leverages token probability insights to reduce the risk of jailbreak attacks. Extensive experiments show that SafeDecoding effectively lowers the success rate and harmfulness of various attacks across multiple LLMs, while maintaining response helpfulness and outperforming existing defense methods.  </p>
              </details>
            
            <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                <!-- 
                
                <a target="_blank" href="https://aclanthology.org/2024.acl-long.303/">[Paper]</a>
                
                
                
                <a target="_blank" href="https://github.com/uw-nsl/SafeDecoding">[Code]</a>
                
                 -->
                
                
                    <a target="_blank" href="https://aclanthology.org/2024.acl-long.303/"> Paper </a>
                
                 | 
                
                
                    <a target="_blank" href="https://github.com/uw-nsl/SafeDecoding"> Code </a>
                
                
                
            </p>
            

            

        </div>
    </div>
</div>

<!-- <div class="row no-gutters d-md-none border-bottom border-gray   lazy" data-src="/assets/images/covers/img_safedecoding.png">
    <div class="w-100" style="background-color: rgba(255,255,255,0.9);">
        <div class="d-flex align-items-start flex-column py-3 px-4">
            <div class="mb-auto"></div>
            <div>
                <h5 class="mt-0 mb-1 font-weight-normal">SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding</h5>
                <p class="mt-0 mb-0 small"><span class="text-body">
            Zhangchen Xu, </span><span class="text-body">
            <strong>Fengqing Jiang</strong>, </span><span class="text-body">
            Luyao Niu, </span><span class="text-body">
            Jinyuan Jia, </span><span class="text-body">
            Bill Yuchen Lin, </span><span class="text-body">
            Radha Poovendran</span></p>
                <p class="mt-0 mb-0 small"><i>ACL 2024</i> 2024 </p>
                <p class="mt-0 mb-0 small text-muted">This paper introduces SafeDecoding, a safety-aware decoding strategy for LLMs that leverages token probability insights to reduce the risk of jailbreak attacks. Extensive experiments show that SafeDecoding effectively lowers the success rate and harmfulness of various attacks across multiple LLMs, while maintaining response helpfulness and outperforming existing defense methods.  </p>
                
                <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                    
                    
                    <a target="_blank" href="https://aclanthology.org/2024.acl-long.303/">[Paper]</a>
                    
                    
                    
                    <a target="_blank" href="https://github.com/uw-nsl/SafeDecoding">[Code]</a>
                    
                    
                </p>
            </div>
        </div>
    </div>

</div> -->

<div class="row no-gutters d-md-none 
     border-bottom 
     border-gray 
      
     ">

  <!-- Full-width column with side margins -->
  <div class="col-12 p-3 pl-md-0 mx-3">

    <h5 class="mt-0 mb-1 font-weight-normal">SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding</h5>
    <p class="mt-0 mb-0 small">
      <span class="text-body">
            Zhangchen Xu, </span><span class="text-body">
            <strong>Fengqing Jiang</strong>, </span><span class="text-body">
            Luyao Niu, </span><span class="text-body">
            Jinyuan Jia, </span><span class="text-body">
            Bill Yuchen Lin, </span><span class="text-body">
            Radha Poovendran</span>
    </p>
    
    <p class="mt-0 mb-0 small">
      <i>ACL 2024</i><strong></strong>
      
    </p>

    <!-- Foldable abstract -->
    <details class="mt-0 mb-0 small text-muted">
      <summary>TL;DR</summary>
      <p>This paper introduces SafeDecoding, a safety-aware decoding strategy for LLMs that leverages token probability insights to reduce the risk of jailbreak attacks. Extensive experiments show that SafeDecoding effectively lowers the success rate and harmfulness of various attacks across multiple LLMs, while maintaining response helpfulness and outperforming existing defense methods.  </p>
    </details>

    <!-- Links with separator -->
    <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
      
        
          <a target="_blank" href="https://aclanthology.org/2024.acl-long.303/">Paper</a>
        
         | 
      
        
          <a target="_blank" href="https://github.com/uw-nsl/SafeDecoding">Code</a>
        
        
      
    </p>

    <!-- Optional media coverage -->
    

  </div><!-- /.col-12 -->
</div><!-- /.row -->
            
                
<div class="d-none d-md-block">
    <div class="row no-gutters border-bottom border-gray">
        <!-- <div class="col-md-3 col-xl-2 mb-md-0 p-md-3"> -->
            <!--<img data-src="/assets/images/covers/img_app.png" alt="Identifying and Mitigating Vulnerabilities in LLM-Integrated Applications" class="lazy w-100 rounded-sm" src="/assets/images/empty_300x200.png">-->
        <!-- </div> -->
        
        <!-- <div class="col-md-12 offset-md-1 col-xl-8 offset-xl-2 p-3 pl-md-0">-->
            <div class="col-12 p-3 pl-md-0 mx-3"> 
            <h5 class="mt-0 mb-1 font-weight-normal">Identifying and Mitigating Vulnerabilities in LLM-Integrated Applications</h5>
            <p class="mt-0 mb-0 small"><span class="text-body">
            <strong>Fengqing Jiang</strong>, </span><span class="text-body">
            Zhangchen Xu, </span><span class="text-body">
            Luyao Niu, </span><span class="text-body">
            Boxin Wang, </span><span class="text-body">
            Jinyuan Jia, </span><span class="text-body">
            Bo Li, </span><span class="text-body">
            Radha Poovendran</span></p>
            <p class="mt-0 mb-0 small"><i>AsiaCCS 2024 (Poster)</i><strong></strong> </p>
            <!-- <p class="mt-0 mb-0 small text-muted">This paper examines new security vulnerabilities in LLM-integrated applications, where malicious insiders or external attackers can manipulate the query-response process to force LLMs (such as GPT-3.5 and GPT-4) into producing biased, toxic, or misleading outputs. To counter these risks, the authors propose a lightweight, threat-agnostic defense that enforces integrity, source identification, attack detectability, and utility preservation, demonstrating its effectiveness through empirical evaluation.</p> -->
            <details class="mt-0 mb-0 small text-muted">
                <summary>TL;DR</summary>
                <p>This paper examines new security vulnerabilities in LLM-integrated applications, where malicious insiders or external attackers can manipulate the query-response process to force LLMs (such as GPT-3.5 and GPT-4) into producing biased, toxic, or misleading outputs. To counter these risks, the authors propose a lightweight, threat-agnostic defense that enforces integrity, source identification, attack detectability, and utility preservation, demonstrating its effectiveness through empirical evaluation.</p>
              </details>
            
            <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                <!-- 
                
                <a target="_blank" href="https://dl.acm.org/doi/abs/10.1145/3634737.3659433">[Paper]</a>
                
                
                
                <a target="_blank" href="https://openreview.net/forum?id=V09d7AMh15">[Full Versioin]</a>
                
                 -->
                
                
                    <a target="_blank" href="https://dl.acm.org/doi/abs/10.1145/3634737.3659433"> Paper </a>
                
                 | 
                
                
                    <a target="_blank" href="https://openreview.net/forum?id=V09d7AMh15"> Full Versioin </a>
                
                
                
            </p>
            

            

        </div>
    </div>
</div>

<!-- <div class="row no-gutters d-md-none border-bottom border-gray   lazy" data-src="/assets/images/covers/img_app.png">
    <div class="w-100" style="background-color: rgba(255,255,255,0.9);">
        <div class="d-flex align-items-start flex-column py-3 px-4">
            <div class="mb-auto"></div>
            <div>
                <h5 class="mt-0 mb-1 font-weight-normal">Identifying and Mitigating Vulnerabilities in LLM-Integrated Applications</h5>
                <p class="mt-0 mb-0 small"><span class="text-body">
            <strong>Fengqing Jiang</strong>, </span><span class="text-body">
            Zhangchen Xu, </span><span class="text-body">
            Luyao Niu, </span><span class="text-body">
            Boxin Wang, </span><span class="text-body">
            Jinyuan Jia, </span><span class="text-body">
            Bo Li, </span><span class="text-body">
            Radha Poovendran</span></p>
                <p class="mt-0 mb-0 small"><i>AsiaCCS 2024 (Poster)</i>  </p>
                <p class="mt-0 mb-0 small text-muted">This paper examines new security vulnerabilities in LLM-integrated applications, where malicious insiders or external attackers can manipulate the query-response process to force LLMs (such as GPT-3.5 and GPT-4) into producing biased, toxic, or misleading outputs. To counter these risks, the authors propose a lightweight, threat-agnostic defense that enforces integrity, source identification, attack detectability, and utility preservation, demonstrating its effectiveness through empirical evaluation.</p>
                
                <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                    
                    
                    <a target="_blank" href="https://dl.acm.org/doi/abs/10.1145/3634737.3659433">[Paper]</a>
                    
                    
                    
                    <a target="_blank" href="https://openreview.net/forum?id=V09d7AMh15">[Full Versioin]</a>
                    
                    
                </p>
            </div>
        </div>
    </div>

</div> -->

<div class="row no-gutters d-md-none 
     border-bottom 
     border-gray 
      
     ">

  <!-- Full-width column with side margins -->
  <div class="col-12 p-3 pl-md-0 mx-3">

    <h5 class="mt-0 mb-1 font-weight-normal">Identifying and Mitigating Vulnerabilities in LLM-Integrated Applications</h5>
    <p class="mt-0 mb-0 small">
      <span class="text-body">
            <strong>Fengqing Jiang</strong>, </span><span class="text-body">
            Zhangchen Xu, </span><span class="text-body">
            Luyao Niu, </span><span class="text-body">
            Boxin Wang, </span><span class="text-body">
            Jinyuan Jia, </span><span class="text-body">
            Bo Li, </span><span class="text-body">
            Radha Poovendran</span>
    </p>
    
    <p class="mt-0 mb-0 small">
      <i>AsiaCCS 2024 (Poster)</i><strong></strong>
      
    </p>

    <!-- Foldable abstract -->
    <details class="mt-0 mb-0 small text-muted">
      <summary>TL;DR</summary>
      <p>This paper examines new security vulnerabilities in LLM-integrated applications, where malicious insiders or external attackers can manipulate the query-response process to force LLMs (such as GPT-3.5 and GPT-4) into producing biased, toxic, or misleading outputs. To counter these risks, the authors propose a lightweight, threat-agnostic defense that enforces integrity, source identification, attack detectability, and utility preservation, demonstrating its effectiveness through empirical evaluation.</p>
    </details>

    <!-- Links with separator -->
    <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
      
        
          <a target="_blank" href="https://dl.acm.org/doi/abs/10.1145/3634737.3659433">Paper</a>
        
         | 
      
        
          <a target="_blank" href="https://openreview.net/forum?id=V09d7AMh15">Full Versioin</a>
        
        
      
    </p>

    <!-- Optional media coverage -->
    

  </div><!-- /.col-12 -->
</div><!-- /.row -->
            
                
<div class="d-none d-md-block">
    <div class="row no-gutters border-bottom border-gray">
        <!-- <div class="col-md-3 col-xl-2 mb-md-0 p-md-3"> -->
            <!--<svg class="bubble-visual-hash lazy w-100 rounded-sm" data-bubble-visual-hash="/publications/2024/24brave" viewBox="0 0 300 200"></svg>-->
        <!-- </div> -->
        
        <!-- <div class="col-md-12 offset-md-1 col-xl-8 offset-xl-2 p-3 pl-md-0">-->
            <div class="col-12 p-3 pl-md-0 mx-3"> 
            <h5 class="mt-0 mb-1 font-weight-normal">Brave: Byzantine-Resilient and Privacy-Preserving Peer-to-Peer Federated Learning</h5>
            <p class="mt-0 mb-0 small"><span class="text-body">
            Zhangchen Xu, </span><span class="text-body">
            <strong>Fengqing Jiang</strong>, </span><span class="text-body">
            Luyao Niu, </span><span class="text-body">
            Jinyuan Jia, </span><span class="text-body">
            Radha Poovendran</span></p>
            <p class="mt-0 mb-0 small"><i>AsiaCCS 2024 (Poster)</i><strong></strong> </p>
            <!-- <p class="mt-0 mb-0 small text-muted">This paper introduces Brave, a protocol for peer-to-peer federated learning that simultaneously preserves privacy against honest-but-curious adversaries and ensures Byzantine resilience. Brave guarantees that malicious participants cannot infer private data and that all benign participants converge to a global model with bounded deviation, achieving competitive accuracy even in adversarial settings.</p> -->
            <details class="mt-0 mb-0 small text-muted">
                <summary>TL;DR</summary>
                <p>This paper introduces Brave, a protocol for peer-to-peer federated learning that simultaneously preserves privacy against honest-but-curious adversaries and ensures Byzantine resilience. Brave guarantees that malicious participants cannot infer private data and that all benign participants converge to a global model with bounded deviation, achieving competitive accuracy even in adversarial settings.</p>
              </details>
            
            <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                <!-- 
                
                <a target="_blank" href="https://dl.acm.org/doi/abs/10.1145/3634737.3659428">[Paper]</a>
                
                
                
                <a target="_blank" href="https://arxiv.org/abs/2401.05562">[Full Version]</a>
                
                 -->
                
                
                    <a target="_blank" href="https://dl.acm.org/doi/abs/10.1145/3634737.3659428"> Paper </a>
                
                 | 
                
                
                    <a target="_blank" href="https://arxiv.org/abs/2401.05562"> Full Version </a>
                
                
                
            </p>
            

            

        </div>
    </div>
</div>

<!-- <div class="row no-gutters d-md-none border-bottom border-gray   " data-src="">
    <div class="w-100" style="background-color: rgba(255,255,255,0.9);">
        <div class="d-flex align-items-start flex-column py-3 px-4">
            <div class="mb-auto"></div>
            <div>
                <h5 class="mt-0 mb-1 font-weight-normal">Brave: Byzantine-Resilient and Privacy-Preserving Peer-to-Peer Federated Learning</h5>
                <p class="mt-0 mb-0 small"><span class="text-body">
            Zhangchen Xu, </span><span class="text-body">
            <strong>Fengqing Jiang</strong>, </span><span class="text-body">
            Luyao Niu, </span><span class="text-body">
            Jinyuan Jia, </span><span class="text-body">
            Radha Poovendran</span></p>
                <p class="mt-0 mb-0 small"><i>AsiaCCS 2024 (Poster)</i> 2024 </p>
                <p class="mt-0 mb-0 small text-muted">This paper introduces Brave, a protocol for peer-to-peer federated learning that simultaneously preserves privacy against honest-but-curious adversaries and ensures Byzantine resilience. Brave guarantees that malicious participants cannot infer private data and that all benign participants converge to a global model with bounded deviation, achieving competitive accuracy even in adversarial settings.</p>
                
                <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                    
                    
                    <a target="_blank" href="https://dl.acm.org/doi/abs/10.1145/3634737.3659428">[Paper]</a>
                    
                    
                    
                    <a target="_blank" href="https://arxiv.org/abs/2401.05562">[Full Version]</a>
                    
                    
                </p>
            </div>
        </div>
    </div>

</div> -->

<div class="row no-gutters d-md-none 
     border-bottom 
     border-gray 
      
     ">

  <!-- Full-width column with side margins -->
  <div class="col-12 p-3 pl-md-0 mx-3">

    <h5 class="mt-0 mb-1 font-weight-normal">Brave: Byzantine-Resilient and Privacy-Preserving Peer-to-Peer Federated Learning</h5>
    <p class="mt-0 mb-0 small">
      <span class="text-body">
            Zhangchen Xu, </span><span class="text-body">
            <strong>Fengqing Jiang</strong>, </span><span class="text-body">
            Luyao Niu, </span><span class="text-body">
            Jinyuan Jia, </span><span class="text-body">
            Radha Poovendran</span>
    </p>
    
    <p class="mt-0 mb-0 small">
      <i>AsiaCCS 2024 (Poster)</i><strong></strong>
      
    </p>

    <!-- Foldable abstract -->
    <details class="mt-0 mb-0 small text-muted">
      <summary>TL;DR</summary>
      <p>This paper introduces Brave, a protocol for peer-to-peer federated learning that simultaneously preserves privacy against honest-but-curious adversaries and ensures Byzantine resilience. Brave guarantees that malicious participants cannot infer private data and that all benign participants converge to a global model with bounded deviation, achieving competitive accuracy even in adversarial settings.</p>
    </details>

    <!-- Links with separator -->
    <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
      
        
          <a target="_blank" href="https://dl.acm.org/doi/abs/10.1145/3634737.3659428">Paper</a>
        
         | 
      
        
          <a target="_blank" href="https://arxiv.org/abs/2401.05562">Full Version</a>
        
        
      
    </p>

    <!-- Optional media coverage -->
    

  </div><!-- /.col-12 -->
</div><!-- /.row -->
            
                
<div class="d-none d-md-block">
    <div class="row no-gutters  border-gray">
        <!-- <div class="col-md-3 col-xl-2 mb-md-0 p-md-3"> -->
            <!--<img data-src="/assets/images/covers/img_badchain.png" alt="BadChain: Backdoor Chain-of-Thought Prompting for Large Language Models" class="lazy w-100 rounded-sm" src="/assets/images/empty_300x200.png">-->
        <!-- </div> -->
        
        <!-- <div class="col-md-12 offset-md-1 col-xl-8 offset-xl-2 p-3 pl-md-0">-->
            <div class="col-12 p-3 pl-md-0 mx-3"> 
            <h5 class="mt-0 mb-1 font-weight-normal">BadChain: Backdoor Chain-of-Thought Prompting for Large Language Models</h5>
            <p class="mt-0 mb-0 small"><span class="text-body">
            Zhen Xiang, </span><span class="text-body">
            <strong>Fengqing Jiang</strong>, </span><span class="text-body">
            Zidi Xiong, </span><span class="text-body">
            Bhaskar Ramasubramanian, </span><span class="text-body">
            Radha Poovendran, </span><span class="text-body">
            Bo Li</span></p>
            <p class="mt-0 mb-0 small"><i>ICLR 2024</i><strong></strong> </p>
            <!-- <p class="mt-0 mb-0 small text-muted">This paper introduces BadChain, a backdoor attack against large language models that exploits chain-of-thought prompting by inserting a malicious reasoning step without needing access to the training data or model parameters. When a backdoor trigger is present in a query, the modified demonstration examples lead the model to output unintended content, with high attack success rates observed especially on models with strong reasoning capabilities like GPT-4.  </p> -->
            <details class="mt-0 mb-0 small text-muted">
                <summary>TL;DR</summary>
                <p>This paper introduces BadChain, a backdoor attack against large language models that exploits chain-of-thought prompting by inserting a malicious reasoning step without needing access to the training data or model parameters. When a backdoor trigger is present in a query, the modified demonstration examples lead the model to output unintended content, with high attack success rates observed especially on models with strong reasoning capabilities like GPT-4.  </p>
              </details>
            
            <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                <!-- 
                
                <a target="_blank" href="https://openreview.net/forum?id=c93SBwz1Ma">[Paper]</a>
                
                
                
                <a target="_blank" href="https://github.com/Django-Jiang/BadChain">[Code]</a>
                
                 -->
                
                
                    <a target="_blank" href="https://openreview.net/forum?id=c93SBwz1Ma"> Paper </a>
                
                 | 
                
                
                    <a target="_blank" href="https://github.com/Django-Jiang/BadChain"> Code </a>
                
                
                
            </p>
            

            

        </div>
    </div>
</div>

<!-- <div class="row no-gutters d-md-none  border-gray  rounded-bottom lazy" data-src="/assets/images/covers/img_badchain.png">
    <div class="w-100" style="background-color: rgba(255,255,255,0.9);">
        <div class="d-flex align-items-start flex-column py-3 px-4">
            <div class="mb-auto"></div>
            <div>
                <h5 class="mt-0 mb-1 font-weight-normal">BadChain: Backdoor Chain-of-Thought Prompting for Large Language Models</h5>
                <p class="mt-0 mb-0 small"><span class="text-body">
            Zhen Xiang, </span><span class="text-body">
            <strong>Fengqing Jiang</strong>, </span><span class="text-body">
            Zidi Xiong, </span><span class="text-body">
            Bhaskar Ramasubramanian, </span><span class="text-body">
            Radha Poovendran, </span><span class="text-body">
            Bo Li</span></p>
                <p class="mt-0 mb-0 small"><i>ICLR 2024</i> 2024 </p>
                <p class="mt-0 mb-0 small text-muted">This paper introduces BadChain, a backdoor attack against large language models that exploits chain-of-thought prompting by inserting a malicious reasoning step without needing access to the training data or model parameters. When a backdoor trigger is present in a query, the modified demonstration examples lead the model to output unintended content, with high attack success rates observed especially on models with strong reasoning capabilities like GPT-4.  </p>
                
                <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                    
                    
                    <a target="_blank" href="https://openreview.net/forum?id=c93SBwz1Ma">[Paper]</a>
                    
                    
                    
                    <a target="_blank" href="https://github.com/Django-Jiang/BadChain">[Code]</a>
                    
                    
                </p>
            </div>
        </div>
    </div>

</div> -->

<div class="row no-gutters d-md-none 
      
     border-gray 
      
     rounded-bottom">

  <!-- Full-width column with side margins -->
  <div class="col-12 p-3 pl-md-0 mx-3">

    <h5 class="mt-0 mb-1 font-weight-normal">BadChain: Backdoor Chain-of-Thought Prompting for Large Language Models</h5>
    <p class="mt-0 mb-0 small">
      <span class="text-body">
            Zhen Xiang, </span><span class="text-body">
            <strong>Fengqing Jiang</strong>, </span><span class="text-body">
            Zidi Xiong, </span><span class="text-body">
            Bhaskar Ramasubramanian, </span><span class="text-body">
            Radha Poovendran, </span><span class="text-body">
            Bo Li</span>
    </p>
    
    <p class="mt-0 mb-0 small">
      <i>ICLR 2024</i><strong></strong>
      
    </p>

    <!-- Foldable abstract -->
    <details class="mt-0 mb-0 small text-muted">
      <summary>TL;DR</summary>
      <p>This paper introduces BadChain, a backdoor attack against large language models that exploits chain-of-thought prompting by inserting a malicious reasoning step without needing access to the training data or model parameters. When a backdoor trigger is present in a query, the modified demonstration examples lead the model to output unintended content, with high attack success rates observed especially on models with strong reasoning capabilities like GPT-4.  </p>
    </details>

    <!-- Links with separator -->
    <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
      
        
          <a target="_blank" href="https://openreview.net/forum?id=c93SBwz1Ma">Paper</a>
        
         | 
      
        
          <a target="_blank" href="https://github.com/Django-Jiang/BadChain">Code</a>
        
        
      
    </p>

    <!-- Optional media coverage -->
    

  </div><!-- /.col-12 -->
</div><!-- /.row -->
            
        </div>
        
        
        <h2 class="pt-4" id="year-2023">2023</h2>
        <div class="my-0 p-0 bg-white shadow-sm rounded-sm">
            
                
<div class="d-none d-md-block">
    <div class="row no-gutters  border-gray">
        <!-- <div class="col-md-3 col-xl-2 mb-md-0 p-md-3"> -->
            <!--<svg class="bubble-visual-hash lazy w-100 rounded-sm" data-bubble-visual-hash="/publications/2023/23MDTD_CCS" viewBox="0 0 300 200"></svg>-->
        <!-- </div> -->
        
        <!-- <div class="col-md-12 offset-md-1 col-xl-8 offset-xl-2 p-3 pl-md-0">-->
            <div class="col-12 p-3 pl-md-0 mx-3"> 
            <h5 class="mt-0 mb-1 font-weight-normal">MDTD: A Multi-Domain Trojan Detector for Deep Neural Networks</h5>
            <p class="mt-0 mb-0 small"><span class="text-body">
            Arezoo Rajabi, </span><span class="text-body">
            Surudhi Asokraj, </span><span class="text-body">
            <strong>Fengqing Jiang</strong>, </span><span class="text-body">
            Luyao Niu, </span><span class="text-body">
            Bhaskar Ramasubramanian, </span><span class="text-body">
            James Ritcey, </span><span class="text-body">
            Radha Poovendran</span></p>
            <p class="mt-0 mb-0 small"><i>ACM CCS 2023</i><strong></strong> </p>
            <!-- <p class="mt-0 mb-0 small text-muted">This paper introduces MDTD, a multi-domain Trojan detector that leverages adversarial learning to estimate an input’s distance from a decision boundary, thereby identifying backdoor-triggered samples across image, audio, and graph-based models. Extensive evaluations show that MDTD can effectively detect various types of Trojan triggers—even under adaptive attacks—while preserving high accuracy on benign inputs.</p> -->
            <details class="mt-0 mb-0 small text-muted">
                <summary>TL;DR</summary>
                <p>This paper introduces MDTD, a multi-domain Trojan detector that leverages adversarial learning to estimate an input’s distance from a decision boundary, thereby identifying backdoor-triggered samples across image, audio, and graph-based models. Extensive evaluations show that MDTD can effectively detect various types of Trojan triggers—even under adaptive attacks—while preserving high accuracy on benign inputs.</p>
              </details>
            
            <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                <!-- 
                
                <a target="_blank" href="https://dl.acm.org/doi/10.1145/3576915.3623082">[Paper]</a>
                
                 -->
                
                
                    <a target="_blank" href="https://dl.acm.org/doi/10.1145/3576915.3623082"> Paper </a>
                
                
                
            </p>
            

            

        </div>
    </div>
</div>

<!-- <div class="row no-gutters d-md-none  border-gray rounded-top rounded-bottom " data-src="">
    <div class="w-100" style="background-color: rgba(255,255,255,0.9);">
        <div class="d-flex align-items-start flex-column py-3 px-4">
            <div class="mb-auto"></div>
            <div>
                <h5 class="mt-0 mb-1 font-weight-normal">MDTD: A Multi-Domain Trojan Detector for Deep Neural Networks</h5>
                <p class="mt-0 mb-0 small"><span class="text-body">
            Arezoo Rajabi, </span><span class="text-body">
            Surudhi Asokraj, </span><span class="text-body">
            <strong>Fengqing Jiang</strong>, </span><span class="text-body">
            Luyao Niu, </span><span class="text-body">
            Bhaskar Ramasubramanian, </span><span class="text-body">
            James Ritcey, </span><span class="text-body">
            Radha Poovendran</span></p>
                <p class="mt-0 mb-0 small"><i>ACM CCS 2023</i> 2023 </p>
                <p class="mt-0 mb-0 small text-muted">This paper introduces MDTD, a multi-domain Trojan detector that leverages adversarial learning to estimate an input’s distance from a decision boundary, thereby identifying backdoor-triggered samples across image, audio, and graph-based models. Extensive evaluations show that MDTD can effectively detect various types of Trojan triggers—even under adaptive attacks—while preserving high accuracy on benign inputs.</p>
                
                <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                    
                    
                    <a target="_blank" href="https://dl.acm.org/doi/10.1145/3576915.3623082">[Paper]</a>
                    
                    
                </p>
            </div>
        </div>
    </div>

</div> -->

<div class="row no-gutters d-md-none 
      
     border-gray 
     rounded-top 
     rounded-bottom">

  <!-- Full-width column with side margins -->
  <div class="col-12 p-3 pl-md-0 mx-3">

    <h5 class="mt-0 mb-1 font-weight-normal">MDTD: A Multi-Domain Trojan Detector for Deep Neural Networks</h5>
    <p class="mt-0 mb-0 small">
      <span class="text-body">
            Arezoo Rajabi, </span><span class="text-body">
            Surudhi Asokraj, </span><span class="text-body">
            <strong>Fengqing Jiang</strong>, </span><span class="text-body">
            Luyao Niu, </span><span class="text-body">
            Bhaskar Ramasubramanian, </span><span class="text-body">
            James Ritcey, </span><span class="text-body">
            Radha Poovendran</span>
    </p>
    
    <p class="mt-0 mb-0 small">
      <i>ACM CCS 2023</i><strong></strong>
      
    </p>

    <!-- Foldable abstract -->
    <details class="mt-0 mb-0 small text-muted">
      <summary>TL;DR</summary>
      <p>This paper introduces MDTD, a multi-domain Trojan detector that leverages adversarial learning to estimate an input’s distance from a decision boundary, thereby identifying backdoor-triggered samples across image, audio, and graph-based models. Extensive evaluations show that MDTD can effectively detect various types of Trojan triggers—even under adaptive attacks—while preserving high accuracy on benign inputs.</p>
    </details>

    <!-- Links with separator -->
    <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
      
        
          <a target="_blank" href="https://dl.acm.org/doi/10.1145/3576915.3623082">Paper</a>
        
        
      
    </p>

    <!-- Optional media coverage -->
    

  </div><!-- /.col-12 -->
</div><!-- /.row -->
            
        </div>
        
        
        <h2 class="pt-4" id="year-2021">2021</h2>
        <div class="my-0 p-0 bg-white shadow-sm rounded-sm">
            
                
<div class="d-none d-md-block">
    <div class="row no-gutters border-bottom border-gray">
        <!-- <div class="col-md-3 col-xl-2 mb-md-0 p-md-3"> -->
            <!--<svg class="bubble-visual-hash lazy w-100 rounded-sm" data-bubble-visual-hash="/publications/2021/21chinesedataset" viewBox="0 0 300 200"></svg>-->
        <!-- </div> -->
        
        <!-- <div class="col-md-12 offset-md-1 col-xl-8 offset-xl-2 p-3 pl-md-0">-->
            <div class="col-12 p-3 pl-md-0 mx-3"> 
            <h5 class="mt-0 mb-1 font-weight-normal">A Chinese Multi-type Complex Questions Answering Dataset over Wikidata</h5>
            <p class="mt-0 mb-0 small"><span class="text-body">
            Jianyun Zou, </span><span class="text-body">
            Min Yang, </span><span class="text-body">
            Lichao Zhang, </span><span class="text-body">
            Yechen Xu, </span><span class="text-body">
            Qifan Pan, </span><span class="text-body">
            <strong>Fengqing Jiang</strong>, </span><span class="text-body">
            Ran Qin, </span><span class="text-body">
            Shushu Wang, </span><span class="text-body">
            Yifan He, </span><span class="text-body">
            Songfang Huang, </span><span class="text-body">
            Zhou Zhao</span></p>
            <p class="mt-0 mb-0 small"><i>Preprint</i><strong></strong> </p>
            <!-- <p class="mt-0 mb-0 small text-muted">This paper introduces CLC-QuAD, the first large-scale dataset for complex Chinese KBQA using Wikidata, addressing the language and diversity limitations of existing resources. It also presents a text-to-SPARQL baseline model capable of handling various complex question types and evaluates current state-of-the-art KBQA models, highlighting challenges specific to Chinese.</p> -->
            <details class="mt-0 mb-0 small text-muted">
                <summary>TL;DR</summary>
                <p>This paper introduces CLC-QuAD, the first large-scale dataset for complex Chinese KBQA using Wikidata, addressing the language and diversity limitations of existing resources. It also presents a text-to-SPARQL baseline model capable of handling various complex question types and evaluates current state-of-the-art KBQA models, highlighting challenges specific to Chinese.</p>
              </details>
            
            <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                <!-- 
                
                <a target="_blank" href="https://arxiv.org/abs/2111.06086">[Paper]</a>
                
                 -->
                
                
                    <a target="_blank" href="https://arxiv.org/abs/2111.06086"> Paper </a>
                
                
                
            </p>
            

            

        </div>
    </div>
</div>

<!-- <div class="row no-gutters d-md-none border-bottom border-gray rounded-top  " data-src="">
    <div class="w-100" style="background-color: rgba(255,255,255,0.9);">
        <div class="d-flex align-items-start flex-column py-3 px-4">
            <div class="mb-auto"></div>
            <div>
                <h5 class="mt-0 mb-1 font-weight-normal">A Chinese Multi-type Complex Questions Answering Dataset over Wikidata</h5>
                <p class="mt-0 mb-0 small"><span class="text-body">
            Jianyun Zou, </span><span class="text-body">
            Min Yang, </span><span class="text-body">
            Lichao Zhang, </span><span class="text-body">
            Yechen Xu, </span><span class="text-body">
            Qifan Pan, </span><span class="text-body">
            <strong>Fengqing Jiang</strong>, </span><span class="text-body">
            Ran Qin, </span><span class="text-body">
            Shushu Wang, </span><span class="text-body">
            Yifan He, </span><span class="text-body">
            Songfang Huang, </span><span class="text-body">
            Zhou Zhao</span></p>
                <p class="mt-0 mb-0 small"><i>Preprint</i> 2021 </p>
                <p class="mt-0 mb-0 small text-muted">This paper introduces CLC-QuAD, the first large-scale dataset for complex Chinese KBQA using Wikidata, addressing the language and diversity limitations of existing resources. It also presents a text-to-SPARQL baseline model capable of handling various complex question types and evaluates current state-of-the-art KBQA models, highlighting challenges specific to Chinese.</p>
                
                <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                    
                    
                    <a target="_blank" href="https://arxiv.org/abs/2111.06086">[Paper]</a>
                    
                    
                </p>
            </div>
        </div>
    </div>

</div> -->

<div class="row no-gutters d-md-none 
     border-bottom 
     border-gray 
     rounded-top 
     ">

  <!-- Full-width column with side margins -->
  <div class="col-12 p-3 pl-md-0 mx-3">

    <h5 class="mt-0 mb-1 font-weight-normal">A Chinese Multi-type Complex Questions Answering Dataset over Wikidata</h5>
    <p class="mt-0 mb-0 small">
      <span class="text-body">
            Jianyun Zou, </span><span class="text-body">
            Min Yang, </span><span class="text-body">
            Lichao Zhang, </span><span class="text-body">
            Yechen Xu, </span><span class="text-body">
            Qifan Pan, </span><span class="text-body">
            <strong>Fengqing Jiang</strong>, </span><span class="text-body">
            Ran Qin, </span><span class="text-body">
            Shushu Wang, </span><span class="text-body">
            Yifan He, </span><span class="text-body">
            Songfang Huang, </span><span class="text-body">
            Zhou Zhao</span>
    </p>
    
    <p class="mt-0 mb-0 small">
      <i>Preprint</i><strong></strong>
      
    </p>

    <!-- Foldable abstract -->
    <details class="mt-0 mb-0 small text-muted">
      <summary>TL;DR</summary>
      <p>This paper introduces CLC-QuAD, the first large-scale dataset for complex Chinese KBQA using Wikidata, addressing the language and diversity limitations of existing resources. It also presents a text-to-SPARQL baseline model capable of handling various complex question types and evaluates current state-of-the-art KBQA models, highlighting challenges specific to Chinese.</p>
    </details>

    <!-- Links with separator -->
    <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
      
        
          <a target="_blank" href="https://arxiv.org/abs/2111.06086">Paper</a>
        
        
      
    </p>

    <!-- Optional media coverage -->
    

  </div><!-- /.col-12 -->
</div><!-- /.row -->
            
                
<div class="d-none d-md-block">
    <div class="row no-gutters  border-gray">
        <!-- <div class="col-md-3 col-xl-2 mb-md-0 p-md-3"> -->
            <!--<svg class="bubble-visual-hash lazy w-100 rounded-sm" data-bubble-visual-hash="/publications/2021/21abz" viewBox="0 0 300 200"></svg>-->
        <!-- </div> -->
        
        <!-- <div class="col-md-12 offset-md-1 col-xl-8 offset-xl-2 p-3 pl-md-0">-->
            <div class="col-12 p-3 pl-md-0 mx-3"> 
            <h5 class="mt-0 mb-1 font-weight-normal">Towards Refinement of Unbounded Parallelism in ASMs Using Concurrency and Reflection</h5>
            <p class="mt-0 mb-0 small"><span class="text-body">
            <strong>Fengqing Jiang</strong>, </span><span class="text-body">
            Neng Xiong, </span><span class="text-body">
            Xinyu Lian, </span><span class="text-body">
            Senén González, </span><span class="text-body">
            Klaus-Dieter Schewe</span></p>
            <p class="mt-0 mb-0 small"><i>8th International Conference on Rigorous State-Based Methods</i><strong></strong> </p>
            <!-- <p class="mt-0 mb-0 small text-muted">This paper introduces a method to integrate the BSP bridging model with MapReduce processing by using a work-stealing approach, where idle processors autonomously select and execute tasks from a pool of open threads. It further generalizes this method by refining unboundedly parallel ASMs into concurrent, reflective BSP-ASMs, allowing individual agents to dynamically adapt their programs.</p> -->
            <details class="mt-0 mb-0 small text-muted">
                <summary>TL;DR</summary>
                <p>This paper introduces a method to integrate the BSP bridging model with MapReduce processing by using a work-stealing approach, where idle processors autonomously select and execute tasks from a pool of open threads. It further generalizes this method by refining unboundedly parallel ASMs into concurrent, reflective BSP-ASMs, allowing individual agents to dynamically adapt their programs.</p>
              </details>
            
            <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                <!-- 
                
                <a target="_blank" href="https://link.springer.com/chapter/10.1007/978-3-030-77543-8_10">[Paper]</a>
                
                 -->
                
                
                    <a target="_blank" href="https://link.springer.com/chapter/10.1007/978-3-030-77543-8_10"> Paper </a>
                
                
                
            </p>
            

            

        </div>
    </div>
</div>

<!-- <div class="row no-gutters d-md-none  border-gray  rounded-bottom " data-src="">
    <div class="w-100" style="background-color: rgba(255,255,255,0.9);">
        <div class="d-flex align-items-start flex-column py-3 px-4">
            <div class="mb-auto"></div>
            <div>
                <h5 class="mt-0 mb-1 font-weight-normal">Towards Refinement of Unbounded Parallelism in ASMs Using Concurrency and Reflection</h5>
                <p class="mt-0 mb-0 small"><span class="text-body">
            <strong>Fengqing Jiang</strong>, </span><span class="text-body">
            Neng Xiong, </span><span class="text-body">
            Xinyu Lian, </span><span class="text-body">
            Senén González, </span><span class="text-body">
            Klaus-Dieter Schewe</span></p>
                <p class="mt-0 mb-0 small"><i>8th International Conference on Rigorous State-Based Methods</i> 2021 </p>
                <p class="mt-0 mb-0 small text-muted">This paper introduces a method to integrate the BSP bridging model with MapReduce processing by using a work-stealing approach, where idle processors autonomously select and execute tasks from a pool of open threads. It further generalizes this method by refining unboundedly parallel ASMs into concurrent, reflective BSP-ASMs, allowing individual agents to dynamically adapt their programs.</p>
                
                <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                    
                    
                    <a target="_blank" href="https://link.springer.com/chapter/10.1007/978-3-030-77543-8_10">[Paper]</a>
                    
                    
                </p>
            </div>
        </div>
    </div>

</div> -->

<div class="row no-gutters d-md-none 
      
     border-gray 
      
     rounded-bottom">

  <!-- Full-width column with side margins -->
  <div class="col-12 p-3 pl-md-0 mx-3">

    <h5 class="mt-0 mb-1 font-weight-normal">Towards Refinement of Unbounded Parallelism in ASMs Using Concurrency and Reflection</h5>
    <p class="mt-0 mb-0 small">
      <span class="text-body">
            <strong>Fengqing Jiang</strong>, </span><span class="text-body">
            Neng Xiong, </span><span class="text-body">
            Xinyu Lian, </span><span class="text-body">
            Senén González, </span><span class="text-body">
            Klaus-Dieter Schewe</span>
    </p>
    
    <p class="mt-0 mb-0 small">
      <i>8th International Conference on Rigorous State-Based Methods</i><strong></strong>
      
    </p>

    <!-- Foldable abstract -->
    <details class="mt-0 mb-0 small text-muted">
      <summary>TL;DR</summary>
      <p>This paper introduces a method to integrate the BSP bridging model with MapReduce processing by using a work-stealing approach, where idle processors autonomously select and execute tasks from a pool of open threads. It further generalizes this method by refining unboundedly parallel ASMs into concurrent, reflective BSP-ASMs, allowing individual agents to dynamically adapt their programs.</p>
    </details>

    <!-- Links with separator -->
    <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
      
        
          <a target="_blank" href="https://link.springer.com/chapter/10.1007/978-3-030-77543-8_10">Paper</a>
        
        
      
    </p>

    <!-- Optional media coverage -->
    

  </div><!-- /.col-12 -->
</div><!-- /.row -->
            
        </div>
        
    </div>

    <div class="col-2 d-none d-lg-block">
        <div id="navbar-year" class="nav nav-pills flex-column sticky-top" style="top: 80px">
            
            <a class="nav-link d-block" href="#year-2025">2025</a>
            
            <a class="nav-link d-block" href="#year-2024">2024</a>
            
            <a class="nav-link d-block" href="#year-2023">2023</a>
            
            <a class="nav-link d-block" href="#year-2021">2021</a>
            
        </div>
    </div>

</div>

    </div>
    <footer class="footer">
    <div class="container">
        <div class="row my-3">
            <div class="col-6">
                <div class="text-muted">
                    <i>Last updated: Feb 2025</i>
                </div>
            </div>
            <div class="col-6">
                <div class="text-right text-muted">
                    
                </div>
            </div>
        </div>
    </div>
</footer>


    <!-- Scripts -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/jquery.lazy/1.7.9/jquery.lazy.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.6.0/js/bootstrap.min.js" integrity="sha512-XKa9Hemdy1Ui3KSGgJdgMyYlUg1gM+QhL6cnlyTe2qzMCYm4nAZ1PsVerQzTTXzonUR+dmswHqgJPuwCq1MaAg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/github-buttons/2.14.2/buttons.min.js" integrity="sha512-OYwZx04hKFeFNYrWxIyo3atgGpb+cxU0ENWBZs72X7T9U+NoHPM1ftUn/Mfw7dRDXrqWA6M1wBg6z6fGE32aeA==" crossorigin="anonymous"></script>
    <script src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
    <script src="https://unpkg.com/imagesloaded@5/imagesloaded.pkgd.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js" integrity="sha384-7zkQWkzuo3B5mTepMUcHkMB5jZaolc2xDwL6VFqjFALcbeS9Ggm/Yr2r3Dy4lfFg" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js" integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
              delimiters: [
                  {left: '$$', right: '$$', display: true},
                  {left: '$', right: '$', display: false}
              ],
              throwOnError : false
            });
        });
    </script>
    <script src="/assets/js/common.js"></script>
    <script src="/assets/js/bubble_visual_hash.js"></script>
</body>
</html>
