---
title:          "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs"
date:           2024-08-11 00:01:00 +0800
selected:       true
# pub:            "62nd Annual Meeting of the Association for Computational Linguistics (ACL)"
pub: "ACL 2024"
pub_date:       "2024"
abstract: >-
  This paper reveals that current LLM safety methods, which assume text is interpreted only semantically, can be bypassed using ASCII art. The authors introduce ArtPrompt, an ASCII art-based jailbreak attack, and the Vision-in-Text Challenge (ViTC) benchmark to demonstrate that leading LLMs struggle with non-semantic cues, enabling attackers to trigger undesired behaviors with black-box access.
  
cover:          /assets/images/covers/img_artprompt.png
authors:
- Fengqing Jiang
- Zhangchen Xu
- Luyao Niu
- Zhen Xiang
- Bhaskar Ramasubramanian
- Bo Li
- Radha Poovendran
links:
  Paper: https://aclanthology.org/2024.acl-long.809/
  Code: https://github.com/uw-nsl/ArtPrompt

medias:
  DeepLearning.AI: https://www.deeplearning.ai/the-batch/artprompt-a-technique-that-exploits-ascii-art-to-bypass-llm-safety-measures/
  X: https://x.com/emollick/status/1763687813386547594
  tom's Hardware: https://www.tomshardware.com/tech-industry/artificial-intelligence/researchers-jailbreak-ai-chatbots-with-ascii-art-artprompt-bypasses-safety-measures-to-unlock-malicious-queries
---