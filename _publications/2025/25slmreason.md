---
title:          "Small Models Struggle to Learn from Strong Reasoners"
date:           2025-06-30 00:01:00 +0800
selected:       true
# pub:            "Thirteenth International Conference on Learning Representations (ICLR)"
pub: "ACL 25 (Finding)"
pub_date:       "2025"
abstract: >-
  This paper reveals that small models (≤3B parameters) struggle with long chain-of-thought reasoning and instead perform better with shorter, simpler reasoning chains that align with their learning capacity. To address this, the authors introduce Mix Distillation—a method that blends long and short reasoning examples—which significantly boosts small model performance compared to using either type alone.
  
cover:          /assets/images/covers/img_magpie.png
authors:
- Yuetai Li
-  Xiang Yue
-   Zhangchen Xu
-    Fengqing Jiang
-     Luyao Niu
-      Bill Yuchen Lin
-       Bhaskar Ramasubramanian
-        Radha Poovendran
links:
  Preprint: https://arxiv.org/abs/2502.12143
  Project Website: https://small-model-gap.github.io/


---