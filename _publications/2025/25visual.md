---
title:         "VisualSphinx: Large-Scale Synthetic Vision Logic Puzzles for RL"
date:           2025-07-02 00:01:00 +0800
selected:       true
pub: "Preprint"
pub_date:       "2025"
abstract: >-
  Vision language models (VLMs) are expected to perform effective multimodal reasoning and make logically coherent decisions, which is critical to tasks such as diagram understanding and spatial problem solving. However, current VLM reasoning lacks large-scale and well-structured training datasets. To bridge this gap, we propose VisualSphinx, a first-of-its-kind large-scale synthetic visual logical reasoning training data. To tackle the challenge of image synthesis with grounding answers, we propose a rule-to-image synthesis pipeline, which extracts and expands puzzle rules from seed questions and generates the code of grounding synthesis image synthesis for puzzle sample assembly. Experiments demonstrate that VLM trained using GRPO on VisualSphinx benefit from logical coherence and readability of our dataset and exhibit improved performance on logical reasoning tasks. The enhanced reasoning capabilities developed from VisualSphinx also benefit other reasoning tasks such as algebraic reasoning, arithmetic reasoning and geometry reasoning.
  
cover:          /assets/images/covers/img_magpie.png
authors:
- Yichen Feng
- Zhangchen Xu
- Fengqing Jiang
- Yuetai Li
- Bhaskar Ramasubramanian
- Luyao Niu
- Bill Yuchen Lin
- Radha Poovendran
links:
  Preprint: https://arxiv.org/abs/2505.23977
  Project Page: https://visualsphinx.github.io/
  Huggingface: https://huggingface.co/VisualSphinx


---